From Chaos to Clarity: Time Series Anomaly
Detection in Astronomical Observations
Xinli Hao

, Yile Chen
y
, Chen Yang
z
, Zhihui Du
x
, Chaohong Ma

, Chao Wu
{
,Xiaofeng Meng#


Renmin University of China, Bejing, China
y
Nanyang Technological University, Singapore
z
China National Clearing Center, Beijing, China
x
New Jersey Institute of Technology, Newark, USA
{
National Astronomical Observatories, CAS, Beijing, China
f
xinli
hao, chaohma, xfmeng
g
@ruc.edu.cn, yile001@e.ntu.edu.sg,
chenyang@cncc.cn, zhihui.du@njit.edu, cwu@bao.ac.cn
Abstract
ŠWith the development of astronomical facilities,
large-scale time series data observed by these facilities is being
collected. Analyzing anomalies in these astronomical observations
is crucial for uncovering potential celestial events and physical
phenomena, thus advancing the scientic research process. How-
ever, existing time series anomaly detection methods fall short in
tackling the unique characteristics of astronomical observations
where each star is inherently independent but interfered by
random concurrent noise, resulting in a high rate of false alarms.
To overcome the challenges, we propose AERO, a novel two-
stage framework tailored for unsupervised anomaly detection
in astronomical observations. In the rst stage, we employ a
Transformer-based encoder-decoder architecture to learn the
normal temporal patterns on each variate (i.e., star) in alignment
with the characteristic of variate independence. In the second
stage, we enhance the graph neural network with a window-wise
graph structure learning to tackle the occurrence of concurrent
noise characterized by spatial and temporal randomness. In
this way, AERO is not only capable of distinguishing normal
temporal patterns from potential anomalies but also effectively
differentiating concurrent noise, thus decreasing the number
of false alarms. We conducted extensive experiments on three
synthetic datasets and three real-world datasets. The results
demonstrate that AERO outperforms the compared baselines.
Notably, compared to the state-of-the-art model, AERO improves
the F1-score by up to 8.76% and 2.63% on synthetic and real-
world datasets respectively.
Index Terms
ŠTime series, Anomaly detection, AI for science
I . I
N T R O D U C T I O N
In recent years, scientic discovery in astronomy has ex-
perienced signicant advancements owing to the development
of modern facilities, such as optical or radio telescopes with
higher temporal and spatial resolution. These cutting-edge
developments have greatly facilitated the collection of large-
scale astronomical data, including optical images and radio
signals, thus providing researchers with valuable resources to
explore and explain the natural world. Meanwhile, the avail-
ability of such extensive datasets highlights the necessity for
automated data analysis and interpretation to enhance related
disciplines. In particular, anomalies (i.e., deviations from the
normal patterns) identied in astronomical observations are
# Corresponding author: Xiaofeng Meng.
Fig. 1. An illustration for multivariate time series obtained from multiple
stars and examples for anomaly detection. (a) Images of stars captured by
telescopes. (b) The magnitudes (i.e. brightness) of stars are extracted. (c)
Multiple magnitudes series constitute a multivariate time series containing
true anomalies and concurrent noise.
critical indicators for potential celestial events or physical
phenomena [
1
]. Given the vast volume of data records, it is
important to design an effective anomaly detection method to
assist in the scientic research process.
The collected astronomical observations of multiple stars
can be represented as a kind of multivariate time series. For
example, as depicted in Fig.
 1
, the magnitude (i.e. brightness)
of a star can be extracted from the image observed by
telescopes. The magnitudes of a given star from a sequence of
observations form a univariate time series. Subsequently, by
aggregating multiple univariate time series, a multivariate time
series can be constructed, which serves as the foundational
data format for further analysis and scientic event discovery.
While sharing the general format of multivariate time series,
the data obtained from astronomical observations possess
unique properties:
1) Variate independence.
In astronomical
multivariate time series, each variate corresponds to the time
series of a star's magnitude. Given that stars are separated by
vast physical distances and characterized by various physical
properties, these variates are considered inherently indepen-
dent and lack mutual inuence. This contrasts with the mul-
tivariate time series from industrial devices [
2
] or IT systems
[
3
], where variates, typically from a single entity such as a
server, exhibit close interdependencies throughout their opera-
arXiv:2403.10220v1  [cs.LG]  15 Mar 2024
Fig. 2. Illustration of the anomaly detection process. (a) existing anomaly
detection (AD) methods fail to distinguish normal data from concurrent noise
in astronomical observations. (b) AERO overcomes the limitations through a
two-stage framework.
tion;
2) Concurrent noise.
Astronomical observations obtained
via telescopes are vulnerable to environmental interferences,
such as shadowing effects from cloud coverage or extreme
weather conditions. When affected by such factors, the mag-
nitudes of a subset of stars exhibit simultaneous uctuations
over a time period, a phenomenon characterized as concurrent
noise (Fig.
 1
(c)). Moreover, the presence of this concurrent
noise is both spatially and temporally random, rendering its
occurrence inh erently unpredictable. However, such noise does
not correspond to actual celestial events, and therefore should
not be interpreted as true anomalies. Unfortunately, these two
unique properties present challenges to existing time series
anomaly detection methods, because they are not designed to
accommodate these two characteristics.
First, to tackle the property of variate independence, it is
natural to employ univariate time series anomaly detection
methods, which focus on modeling each variate separately
[
4
]. While these methods are well-suited to the aspect of
variate independence, they fail to identify the concurrent noise
due to their inability to capture correlations across multiple
stars. As a result, such noise might be mistakenly classied
as true anomalies (celestial events), leading to an increased
rate of false positives (Fig.
 2
(a)). Conversely, to tackle the
property of concurrent noise, it is necessary to treat the
magnitudes series of multiple stars jointly as a multivariate
time series. However, existing methods for multivariate time
series anomaly detection are primarily designed for systems,
such as industry devices and urban infrastructures [
2
], [
5
],
[
6
]. They operate under the assumption of persistent and pre-
dictable correlations among all variates. Such an assumption
contradicts the inherent independence characteristic among
stars during the absence of concurrent noise. Therefore, these
methods result in suboptimal performance when applied to
astronomical observations.
Second, recent studies have proposed to treat each variate
in multivariate time series as a node, and applied graph neural
networks (GNNs) to uncover latent dependencies among these
variates [
7
]Œ[
9
]. While these methods demonstrate enhanced
performance in capturing latent dependencies among variates
compared to conventional multivariate time series methods,
they still encounter issues in effectively modeling concurrent
noise. Specically, GNNs in these methods are developed to
explicitly learn the structure of either a global static graph
or a dynamic temp oral graph. As mentioned previously, the
intrinsic unpredictability of environmental interferences indi-
cates that concurrent noise exhibits both spatial and temporal
randomness. In other words, concurrent noise can affect an un-
determined number of stars within any region while occurring
at any time. This suggests the absence of stable dependencies
among variates and the lack of predictable, xed temporal
evolution patterns. Such randomness not only undermines
the basic premise of constructing a static graph, but also
challenges the core rationale behind the implementation of
learning a dynamic graph.
To overcome these challenges, we propose a novel method
named AERO for
A
nomaly d
E
tection in ast
R
onomical
O
bservations in an unsupervised manner. Our method employs
a two-stage detection framework to address the limitations in
both types of existing anomaly detection methods. In the rst
stage, we adopt a Transformer-based encoder-decoder model,
and apply it independently to each variate to reconstruct
univariate time series. This strategy encodes prior knowledge
of variate independence, thus enablin g the model to learn
the normal temporal patterns of stars effectively. Through
this process, potential anomalies with large reconstruction
errors can be preliminarily identied. In the second stage,
based on the results from the rst stage, we consider the
variate dependencies to further differentiate true anomalies
and concurrent noise, as shown in Fig.
 2
(b). To further
distinguish concurrent noise and tackle its spatial and temporal
randomness feature, we integrate GNN with a novel window-
wise graph structure learning technique. This module leverages
the information from the potential anomalies identied in the
rst stage, and avoids the assumptions of stable dependencies
among stars or xed evolution patterns applied in previous
methods. By doing this, our model is b etter equipped to
focus on reconstructing concurrent noise, thus enhancing its
effectiveness in the context of astronomical observations.
The contributions of our work are summarized as follows:

We propose AERO, a novel two-stage time series anomaly
detection method tailored for tackling unique characteristics
in astronomical observations. To the best of our knowledge,
we are the rst to systematically identify and address the
challenges in astronomical time series anomaly detection.

The proposed AERO employs a Transformer-based encoder-
decoder on each variate to select anomaly candidates in
alignment with the variate independence property. Then it
enhances GNN with a window-wise graph structure learn-
ing technique to effectively adapt to the concurrent noise
property.

We conduct extensive experiments on both synthetic datasets
and real-world datasets, the latter obtained from the National
Astronomical Observatories of China. Experiments against
11 baseline methods demonstrate that AERO outperforms
them in most cases and achieves the highest F1-scores.
I I . R
E L AT E D
W
O R K
In this section, we present a range of anomaly detection
methods relevant to our proposed method. First, we rst intro-
duce typical anomaly detection techniques that are applicable
to univariate and multivariate time series. Then we concentrate
on specialized methods that utilize GNN and Transformer
architectures.
A. Time Series Anomaly Detection
Existing studies can be categorized into two groups of
techniques, based on their applicability on either univariate
or multivariate time series data.
Univariate techniques
focus on analyzing a single variate or
treating each variate in time series separately, while neglecting
their potential correlations. Early studies in this area typically
utilize statistical methods. SPOT [
10
] employs Extreme Value
Theory (EVT) to identify outliers of extreme values in stream-
ing univariate time series, bypassing the need for predened
thresholds or assumptions about data distribution. Building
on this, FluxEV extends SPOT [
11
] by identifying not only
extreme values but also abnormal patterns through uctuation
extraction and smoothing processes. In addition, SR [
12
]
adapts the spectral residual model from computer vision to
anomaly detection in industrial services. With the advances
in deep learning, numerous methods have been developed for
anomaly detection based on various models, such as Long
Short Term Memory (LSTM) [
13
], [
14
] and variational auto-
encoder (VAE) [
15
], [
16
]. Moreover, VAE-LSTM [
17
] is a
hybrid method that combines VAE for robust local feature
extraction over short windows with LSTM for long-term
correlation modeling. However, as discussed in Sec.
 I
, these
univariate techniques fail to address the issue of concurrent
noise encountered in astronomical data scenarios.
Multivariate techniques
consider a collection of variates
in time series as a unied entity. These methods usually
model the normal temporal patterns by considering both inter-
variate dependencies and variate-specic behaviors, and iden-
tify anomalies when data points exhibit large reconstruction er-
rors. Notably, LSTM-NDT [
18
] incorporates a nonparametric
anomaly thresholding approach into LSTM for anomaly detec-
tion in spacecraft monitoring systems. MSCRED [
6
] applies
an attention-based convolutional LSTM model for anomaly
detection and diagnosis in power plants. OmniAnomaly [
19
]
is the rst to explicitly account for both temporal dependency
and variable stochasticity using VAE. Building upon this
idea, InterFusion [
3
] combines inter-metric correlation and
temporal dependency through a hierarchical VAE framework.
VQRAEs [
20
] further enables this structure by incorporating
bi-directional capability. In addition, generative adversarial
networks (GANs) have been also adapted for anomaly de-
tection in methods like MAD-GAN [
21
] and DAEMON [
22
].
TimesNet [
23
] is developed as a foundation model applicable
to both univariate and multivariate time series anomaly de-
tection tasks. It introduces an approach of transforming time
series from 1D to 2D space using Fast Fourier Transform
(FFT) and then applies convolution operations to capture tem-
poral and variate dependencies. While these methods consider
multiple time series as a collective unit, none of them address
the issue of concurrent noise. Consequently, they fall short of
mitigating false positives caused by this issue.
B. GNNs for Time Series Anomaly Detection
Considering the dependencies among different variates in
time series, GNNs [
24
], which treat each variate as a node and
employ message passing between nodes, have been utilized in
multiple time series tasks [
25
], including time series anomaly
detection. GDN [
7
] assumes that variate dependencies (i.e.
graph structure) do not change over time, and proposes to
capture the static dependencies through embedding learning
for each variate. In line with this assumption, several network
structures, such as MTAD-GAT [
26
], Stgat-Mad [
27
], MT-
GNN [
28
], and RGSL [
29
], are developed to learn a static,
and globally optimal static graph for multivariate time series.
On the other hand, other methods argue that dependencies
among variates and dynamic and evolve over time. These
methods, including Event2Graph [
9
], GraphAD [
30
], BrainNet
[
31
], METRO [
8
], ESG [
32
], and TSAT [
33
], aim to perform
dynamic graph structure learning. They achieve this by con-
structing graphs at every timestamp and employing sequential
modeling on the graph structures using RNN [
8
], [
9
], [
32
]
or Transformer [
33
]. Bridging these two assumptions, SRD
[
34
] decomposes variate dependencies into a static graph and
is supplemented by dynamic variations unique to individual
samples. While all these methods can selectively learn depen-
dencies among variates, they are not capable of modeling the
spatial and temporal randomness characteristic of concurrent
noise.
C. Transformer for Time Series Anomaly Detection
The success of Transformer [
35
] in sequence modeling has
signicantly inuenced its adoption for time series anomaly
detection [
36
]. Compared to VAE-based methods, Transformer
architecture demonstrates its superiority as both an encoder
and decoder to account for temporal modeling. Based on this,
Transformer variants have been developed in several meth-
ods to enhance anomaly detection capabilities. For example,
TranAD [
37
] enhances the standard Transformer by integrating
self-conditioning and adversarial training processes to improve
performance in anomaly detection. AnomalyTransformer [
38
]
employs a specialized anomaly attention mechanism to bet-
ter distinguish normal and abnormal in both univariate and
multivariate time series. GTA [
5
] augments the input in the
Transformer with graph convolutions and hierarchical dilated
convolutions to capture variate dependencies and temporal
patterns more effectively. Unfortunately, similar to other time
series anomaly detection methods, these methods still face
challenges in being adequately adapted for astronomical ob-
servation scenarios.
I I I . P
R O P O S E D
M
E T H O D
In this section, we rst formulate the problem of anomaly
detection in astronomical observations. Next, we introduce an
Fig. 3. Data format for astronomical observations, which includes
N
variates
(series of magnitudes from
N
stars) over
C T
timestamps. A sliding window
of length
W
is used to partition the complete time series into instances as
X
t
2
R
N

W
.
overview of our method, followed by detailed descriptions of
each component. Finally, we describe the ofine training and
online detection process.
A. Problem Statement
Astronomical observation data can be represented as a time
series, which consists of
N
variates (series of magnitudes from
N
stars) over
C T
timestamps, denoted as:
T
=
f
x
1
;
  
; x
C T
g
where each datapoint
x
t
is collected at a specic timestamp
t
and
x
t
=
f
x
(1)
t
; x
(2)
t
;
  
; x
(
N
)
t
g 2
R
N
denotes the magni-
tudes of
N
stars at time
t
.
Following the practices in previous studies [
3
], [
37
], instead
of directly utilizing the raw time series
T
as training input,
we partition the entire time series into multiple instances by
employing a sliding window of length
W
. As illustrated in
Fig.
 3
, for a given timestamp
t
, a window of length
W
generates the instance
X
t
as follows:
X
t
=
f
x
t

W
+1
;
  
; x
t
g 2
R
N

W
In this way, the raw time series
T
is transformed into a
collection of instances
X
=
f
X
W
; X
W
+1
;
  
; X
C T
g
to serve
as the data for model training.
Our objective is to determine whether an observation
x
(
n
)
t
for each star at every timestamp is anomalous or not in an
unsupervised manner. To achieve this, we aim to learn a
predictive function:
F
(
X
t
)
! O
t
where
O
t
2 f
0
;
1
g
N
denotes the binary an omaly labels for N
variates at timestamp
t
. By aggregating the results from each
instance within the sliding window collection
X
, we are able
to obtain the predictions for the complete time series
T
.
B. Overview
The framework of AERO is illustrated in Fig.
 4
. Building
on the concept of reconstruction-based anomaly detection,
AERO is composed of two modules, namely the temporal
reconstruction module and concurrent noise reconstruction
module, which are specically designed to address the two
distinct properties (i.e., variate independence and concurrent
noise) in astronomical observations.
The temporal reconstruction module utilizes a Transformer-
based encoder-decoder architecture to model the normal tem-
poral patterns of each star. In accordance with the variate
independence property, it treats multiple variates as indepen-
dent univariate time series through a shared network. Fo r a
given instance with a window length
W
from a variate, this
module further employs a shorter window with length
!
for
reconstruction. This approach ensures that the reconstruction
is more focused on the latter parts of the time series while
leveraging a longer context to capture temporal patterns better.
Such a manner aligns well with the inference stage of anomaly
detection at the last timestamp for each instance, as described
in Sec.
 III-A
. Through this module, anomaly candidates are
initially identied based on large reconstruction errors.
The concurrent noise reconstruction module aims to further
lter out instances affected by concurrent noise. To achieve
this, it models the reconstruction errors from the temporal
reconstruction module using a novel graph structure learning
technique within GNN. Given the spatial and temporal ran-
domness characteristic of concurrent noise, we devise a simple
yet effective window-wise graph structure learning technique,
which allows for the generation of a distinct adjacent matrix
for every time window. This technique avoids the GNN's
tendency to learn stable spatial correlations or predictable
temporal patterns in previous methods, which are inconsistent
with the randomness in concurrent noise.
Through the integration of these two modules, AERO
prociently reconstructs both normal temporal patterns and
concurrent noise. The nal anomaly detection results produced
by AERO are the combination of both modules. Compared to
existing methods, the proposed two-stage framework, tailored
for astronomical observations, signicantly reduces the num-
ber of false positives in practical applications.
C. Temporal Reconstruction Module
Transformer has demonstrated its effectiveness in modeling
sequential data in various tasks [
39
], [
40
], and therefore
has been recently adopted in time series anomaly detection
[
36
]. Its performance surpasses previous reconstruction-based
encoder-decoder backbones, such as VAE and RNN [
37
]. In
light of this, we utilize a modied Transformer architecture
as a temporal reconstruction module to learn normal temporal
patterns through a reconstruction process for each variate. As
depicted in Fig.
 4
(b), this module consists of ve components,
including time embedding, input embedding, encoder, decoder,
and output layer.
Time Embedding.
The standard Transformer utilizes posi-
tional encoding to integrate order information into a sequence
with the latent assumption that intervals between consecutive
steps are equal. However, this assumption is not applicable to
astronomical observations, which are usually recorded with
irregular intervals. To consider irregular intervals of obser-
vations, we propose to adopt an enhanced time encoding
technique that incorporates not only the absolute positions
in the original trigonometric function but also encodes time
intervals as learnable phase shifts [
41
], [
42
]. Following the
Fig. 4. An illustration of AERO. (a) The overview framework of AERO. A multivariate time series representing magnitudes series of multiple stars is rst
divided into univariate time series as the input of the temporal reconstruction module. The initial reconstruction errors from the rst module are concatenated
as the input of the concurrent noise reconstruction module. (b) Details of temporal reconstruction module. (c) Details of concurrent noise reconstruction
module.
practice in [
37
], we sum sin and cos terms as the nal time
embedding function, which produces improved performance.
Then the
j
-th dimension of time embedding
T E
t
at timestamp
t
is dened as:
T E
j
t
=
sin
(
f
j

pos
t
+

j


t
)
+
cos
(
f
j

pos
t
+

j


t
)
(1)
where
f
j
represents the pre-dened angle frequency, calcu-
lated as
f
j
= (1
=
10000)
j =d
m
; j
2
[0
; d
m
]
,
d
m
is the dimen-
sions of hidden states in Transformer,
pos
t
is the absolute
position of timestamp
t
,

t
is the time interval between the
current timestamp and the previous one, and

j
is a learnable
parameter.
Input Embedding.
For the reconstruction process, we
generate two types of model inputs,
X
t
2
R
N

W
derived
from long sliding windows, and its subsequence
Y
t
in the latter
part with a short window length of
!
(
! < W
)
, written as
follow:
Y
t
=
f
x
t

!
+1
;
  
; x
t
g 2
R
N

!
(2)
The rationale behind this strategy is aligned with the infer-
ence stage of time series anomaly detection, where anomaly
scores are progressively determined for the last timestamps
using sliding windows. Consequently, our primary focus is
on the reconstruction of the latter parts of a time series (i.e.,
Y
t
), while still leveraging a longer context (i.e.,
X
t
) for the
effective modeling of temporal patterns.
Incorporating the prior knowledge of variate independence
in astronomical observations, we model
X
t
and
Y
t
separately
as
L
(
n
)
t
2
R
1

W
and
S
(
n
)
t
2
R
1

!
as follows:
L
(
n
)
t
=
X
(
n
)
t
=
n
x
(
n
)
t

W
+1
; : : : ; x
(
n
)
t
o
;
S
(
n
)
t
=
Y
(
n
)
t
=
n
x
(
n
)
t

!
+1
; : : : ; x
(
n
)
t
o
:
(3)
Then,
L
(
n
)
t
and
S
(
n
)
t
are projected into an
d
m
dimensional
embedding by a linear projection. We add the time embedding
T E
t
to the projected time series embeddings as the nal input
embedding
I E
(
n
)
t
and
I D
(
n
)
t
as follows:
I E
(
n
)
t
=
W
E

L
(
n
)
t
+
T E
t
;
I D
(
n
)
t
=
W
D

S
(
n
)
t
+
T E
t
:
(4)
To main tain clarity while simplifying the discussion, we
omit superscripts denoting variates and subscripts denoting
timestamps. Therefore, we substitute
I
E
and
I
D
for
I E
(
n
)
t
and
I D
(
n
)
t
respectively in our subsequent explanations.
Encoder.
The encoder produces the representations based
on the time series of long window size. The representations
are generated based on the self-attention mechanisms applied
in Transformer architecture. Specically, the self-attention
mechanisms perform the following operations:
Attention
(
Q; K ; V
) =
sof tmax
(
QK
T
p
d
m
)
V :
(5)
where
Q
,
K
, and
V
represent the query, key, and value matrix
respectively derived from a linear projection on the input
embeddings.
In our work, we adopt multi-head self-attention to model the
embeddings of time series. Specically, the input embeddings
are projected into
h
sets of different queries, keys, and values
to perform self-attention mechanism, which has been shown
to achieve better performance. Given the input representations
I
E
, the output representations of multi-head self-attention are
produced as follows:
M H A
(
Q; K ; V
) =
C oncat
(
H
1
; :::; H
h
)

W
O
H
i
=
Attention
(
Q
i
; K
i
; V
i
)
;
(6)
where
Q
i
,
K
i
, and
V
i
for
i
2 f
1
; : : : ; h
g
are obtained by
passing input
I
E
through projection matrices
W
i
Q
,
W
i
K
,
W
i
V
2
R
d
m

d
m
=h
for each head
h
,
W
O
is learnable parameter matrix.
We follow standard Transformer architecture to combine the
residual connection and layer normalization with multi-head
self-attention, which can be expressed as follows:
M
E
=
Lay er N or m
(
I
E
+
M H A
(
I
E
; I
E
; I
E
))
;
O
E
=
Lay er N or m
(
M
E
+
F F N
(
M
E
))
:
(7)
where
Lay er N or m
denotes layer normalization operation,
F F N
represents the feed-forward neural networks, and
O
E
2
R
d
m

W
is the nal output representations derived from the
encoder.
Decoder.
The objective of the decoder is to reconstruct
the time series with a shorter window length. To achieve
this, the decoder takes the embeddings
I
D
, and the output
representations
O
E
of the encoder as contextual information
to reconstruct the time series for each variate. Specically, the
decoder performs the following operations:
M
D
=
Lay er N or m
(
I
D
+
M H A
(
I
D
; I
D
; I
D
))
;
O
0
D
=
Lay er N or m
(
M
D
+
M H A
(
M
D
; O
E
; O
E
))
(8)
The representations from the encoder are used as values and
keys for the queries generated by shorter windows to capture
temporal patterns within the long context.
Finally, feedforward neural networks and sigmoid activation
are employed to generate the normalized predictions of a
variate:
O
D
=
S ig moid
(
F F N
(
O
0
D
))
:
(9)
Output Layer.
In the output layer, we concatenate the result
O
D
2
R
1

!
from each variate to produce a matrix, denoted
as
^
Y
1
2
R
N

!
, as a reconstructed multivariate time series:
^
Y
1
=
C oncat
(
O
(1)
D
; O
(2)
D
; : : : ; O
(
N
)
D
)
:
(10)
Subsequently, we calculate the initial reconstruction errors
E
2
R
N

!
as follows:
E
=
Y

^
Y
1
:
(11)
By do ing this, anomaly candidates are identied by rela-
tively large errors, whereas normal patterns are characterized
by smaller errors.
D. Concurrent Noise Reconstruction
The temporal reconstruction module is effective at detecting
anomalies on a variate-wise basis. However, there is a high
likelihood of concurrent noise being mistakenly classied as
anomalies if the correlations among variates are not taken
into account. In this module, we aim to rene this issue
by ltering out concurrent noise from the identied anomaly
candidates. Since concurrent noise tends to manifest large
errors simultaneously across multiple variates, it can typically
be distinguished through the modeling of variate correlations.
As illustrated in Fig.
 4
(c), we further rene the anomaly de-
tection process by reconstructing the errors obtained from the
temporal reconstruction module. This is achieved by applying
GNN enhanced with a window-wise graph structure learning
technique detailed as follows.
Window-wise Graph Structure Learning.
Concurrent
noise exhibits characteristics of spatial and temporal ran-
domness, implying that it can appear on an unpredictable
number of stars at any time period. Such characteristics
render the application of existing static or dynamic GNN
methods unsuitable. Static GNN methods rely on a static graph
structure to represent stable dependencies among variates,
while dynamic GNN methods operate under the assumption
of predictable evolving patterns in variate correlations. To
overcome the limitations, we propose a novel approach that
involves constructing a graph structure specic to each sliding
window. Specically, we leverage the errors generated by the
temporal reconstruction module, denoted as
E
t
2
R
N

!
, as
the embedding for the sliding window at timestamp
t
. Then
we compute the similarity between variates
m
and
n
in
E
t
as
follows:
sim
mn
t
=
(
E
(
m
)
t
)
T
E
(
n
)
t



E
(
m
)
t







E
(
n
)
t



(12)
Based on this, the graph structure corresponding to the
window at timestamp
t
is determined by an adjacency matrix
A
t
, which indicates the pairwise similarity between variates:
A
mn
t
=
sim
mn
t
(13)
Reconstruction via GCN.
After deriving the graph struc-
ture for each sliding window, variates exhibiting similar error
patterns are assigned large weights within the graph. As
illustrated in Fig.
 1
, variates affected by concurrent noise
demonstrate simultaneous uctuations, whereas true anomalies
usually exhibit distinct temporal deviations. This principle is
critical in distinguishing concurrent noise from true anomalies.
In other words, if a variate is inuenced by concurrent noise
within a given window, it can be effectively reconstructed
using the error patterns of other similarly affected variates.
This principle, however, does not apply to true anomalies.
Leveraging this insight, we employ a GNN to perform message
passing among variates for concurrent noise reconstruction.
The GNN is implemented as follows:
^
Y
2
=
˙
((
~
D

1
~
AY
t
)
W

+
b

)
(14)
where
Y
t
is the input with the short window size in Eq. (
2
),
~
A
=
A

I
,
~
D
is the degree matrix, and
~
D
mm
=
P
n
~
A
mn
,
˙
is the activation function, and
W

and
b

are learnable param-
eters. It is important to note that self-loops are intentionally
removed in message passing to exclude the information of
the target node itself. This design avoids the situations in
which true anomalies are well reconstructed based on their
own information.
E. Ofine Two-Stage Model Training
To separately focus on modeling normal temporal patterns
and concurrent noise, the training of two modules in AERO
is sequentially arranged in two stages. The overall training
process is described in Algorithm
 1
.
Stage 1: Normal Temporal Pattern Reconstruction.
In
the rst stage, we train the temporal reconstruction module
After this stage, concurrent noise and true anomaly become
prominent by large reconstruction errors. When the loss of the
rst module does not decrease over several patient epochs, we
stop training the temporal reconstruction module and enter the
second stage to handle concurrent noise. The loss function of
the rst stage is:
l oss
r ec
=
Y

^
Y
1
(15)
Stage 2: Concurrent Noise Reconstruction.
In the second
stage, we train the concurrent noise reconstruction module
while freezing the parameters of the rst module to maintain
stable training. The loss function is dened as:
l oss
noise
=
Y

^
Y
1

^
Y
2
(16)
F. Online Detection and Diagnosis
After the model training, we can perform online anomaly
detection, which is summarized in Algorithm
 2
. During the in-
ference phase, AERO operates in an online mode by following
the patterns adopted in the training stage. This involves main-
taining a sliding window with a stride 1. As new observations
arrive, they are appended to the preceding window. Then the
model determines whether the data points are anomalous or
not based on the anomaly scores, which are dened as the
combination of reconstruction errors in two modules:
s
t
=
S
(
Y

^
Y
1

^
Y
2
)
:
(17)
where
S
()
represents the indexing function that selects
the last timestamp to produce
s
t
2
R
N
. Then for each
variate, a higher an omaly score
s
(
n
)
t
2
R
indicates that the
corresponding input
x
(
n
)
t
is hard to reconstruct, and thus more
likely to be an anomaly.
Based on the anomaly scores for
x
(
n
)
t
, we can derive point-
wise anomaly labels for each variate. Specically, if
s
(
n
)
t
is
larger than a threshold, the corresponding
x
(
n
)
t
is marked as an
anomaly. We utilize the widely adopted Peak Over Threshold
(POT) method in previous studies [
10
], [
12
], [
19
], [
37
] to
automatically determine the threshold. The nal anomaly label
is derived by:
O
(
n
)
t
=
1
(
s
(
n
)
t

P O T
(
s
))
(18)
where
s
utilized in the POT method is the collection of all
anomaly scores o btained from the training instances, and
s
(
n
)
t
denotes the anomaly score for variate
n
at the current time
t
.
I V. E
X P E R I M E N T S
In this section, we conduct experiments to answer the
following research questions:

RQ1:
Can our method outperform baselines for anomaly de-
tection in astronomical observations by improving precision
while guaranteeing recall?

RQ2:
How do different components of our method benet
the performance?

RQ3:
How does our method perform in terms of efciency
and scalability?
Algorithm 1
Model Training Process
Input
:
X
and
Y
split from
T
using sliding windows
Require
: Temporal Reconstruction Module
M
1
,
Concurrent Noise Reconstruction Module
M
2
,
epoch
1
and
epoch
2
decided by early stop mechanism.
Output
: Trained
M
1
and
M
2
1:
for
i
= 0
to
epoch
1
do
2:
^
Y
1
 
M
1
(
X ; Y
)
3:
l oss
1
=
Y

^
Y
1
4:
 Update parameter of
M
1
by
l oss
1
.
5:
end for
6:
for
i
=
epoch
1
to
epoch
2
do
7:
^
Y
2
 
M
2
(
Y

^
Y
1
; Y
)
8:
l oss
2
=
Y

^
Y
1

^
Y
2
9:
 Update parameter of
M
2
by
l oss
2
.
10:
end for
Algorithm 2
Online Detection Process
Input
:
X
and
Y
split from test dataset using sliding windows
Require
: Trained
M
1
and
M
2
.
Output
: Predicted label matrix
O
.
1:
for
t
at each timestamp
do
2:
^
Y
1
 
M
1
(
X ; Y
)
3:
^
Y
2
 
M
2
(
Y

^
Y
1
; Y
)
4:
s
t
=
S
(
Y

^
Y
1

^
Y
2
)
5:
for
each variate
n
do
6:
O
(
n
)
t
=
1
(
s
(
n
)
t

P O T
(
s
))
7:
end for
8:
end for

RQ4:
Can the learned graph structure in our method effec-
tively represent the concurrent noise? How does each step
of our method contribute to the anomaly scores?

RQ5:
How do the hyperparameters and congurations in-
uence our method performance?
A. Datasets
We utilize six datasets, including three synthetic datasets
and three real-world datasets, to evaluate the performance of
compared methods.
Synthetic Datasets.
Astronomical observations are unique
in exhibiting characteristics of variate independence and con-
current noise, which usually do not exist in other data domains.
To showcase the effectiveness of our method, we generate
three synthetic datasets in which these properties are injected
into time series. The construction of these datasets begins with
the generation o f basic signals. These signals either conform
to a Gaussian distribution,
X
˘
N
(0
;
0
:
2
2
)
, to simulate the
behavior of non-variable stars, or obey a sinusoidal function
with added Gaussian noise, thereby imitating the behavior of
variable stars. The employed sinusoidal function is as follows:
f
(
t; T
) = 2

sin
(
2

ˇ
T

pos
t
)
where
pos
t
is the absolute position of the current timestamp
and
T
denotes the cycle value sampled from a range between
100 and 300 simulating various variable stars.
Then three types of concurrent noise are injected into the
basic signals. The rst type is data drift, which is simulated
by increasing or decreasing the mean value in the basic
signals. The second type represents the process of darkening
TABLE I
D
ATA S E T
S
TAT I S T I C S
.
Dataset #train #test
#vari-
ates
A
nomaly
(%)
N
oise
(%)
A/N
#Anomaly
Segments
#Noise
variates
SyntheticMiddle 4000 4000 24 0.180 1.719 0.105 5 17/24
SyntheticHigh 4000 4000 24 0.359 1.719 0.209 10 17/24
SyntheticLow 4000 4000 24 0.180 3.438 0.052 5 17/24
AstrosetsMiddle 5540 5387 54 0.153 4.173 0.037 2 54/54
AstrosetsHigh 8000 6117 38 0.117 2.405 0.049 2 38/38
AstrosetsLow 6255 2950 40 0.190 8.419 0.023 6 40/40
*Anomaly(%) represents the proportion of anomalous data points.
*Noise(%) is the proportion of data points affected by concurrent noise.
*A/N denotes the anomaly-to-noise ratio which measures the ratio of true
anomalies in the potential candidates.
*#Noise variates is the number of variates affected by concurrent noise.
followed by recovery, which is induced by occlusions such
as cloud cover. This effect is simulated by adding half a
period of trigonometric function to the basic signals at specic
timestamps. The third type represents the brightening effect,
caused by the sunrise in the morning. This effect is simulated
by adding exponential functions to the basic signals for a
period of time.
As for the injected true anomalies, we use two categories
in astronomical classication datasets from kaggle
1
and are
function in [
43
]. Examples of true anomalies are shown in
Fig.
 5
. We randomly inject the true anomalies to the basic
signals across various variates to create the SyntheticMiddle
dataset with a moderate anomaly-to-noise ratio (A/N). Simi-
larly, we generate additional synthetic datasets with varying
A/N ratios. Specically, we either double the number of
anomalous data points, or the amount of concurrent noise
to create the SyntheticHigh and SyntheticLow datasets. The
statistics of the datasets are summarized in Table
 I
.
Fig. 5. Examples of Injected True Anomalies.
Real-world Datasets.
Given the absence of public datasets
for anomaly detection in astronomical observations, we have
curated three real-world datasets, referred to as Astrosets.
These datasets are processed from the astronomical observa-
tions conducted by the Ground-based Wide Angle Cameras
(GWAC) [
44
] at the National Astronomical Observatories of
China. The three datasets are selected to contain a broad range
of statistical characteristics, as presented in Table
 I
. Moreover,
different from other anomaly detection datasets, anomalies in
real astronomical observations are relatively rare. In light of
this, we specically annotate the number of segments where
anomalies occur to illustrate the datasets.
1
https://www.kaggle.com/competitions/PLAsTiCC-2018/data
B. Baselines and Experimental Setup
We compare the performance of our proposed AERO
2
with
ten methods for time series anomaly detection, including
ve univariate methods (i.e., Template Matching, SR, SPOT,
FluxEV, and Donut) and six multivariate methods (i.e., Om-
niAnomaly, AnomalyTransformer, TranAD, GDN, ESG, and
TimesNet). Details of these baselines are provided as follows:

Template Matching (TM) [
45
]: it is a supervised method
for celestial event discovery in astronomy. It employs pre-
dened event templates to match newly arrived data. We
treat the templates as anomalies in the experiments.

SR
[
12
]: it is a univariate time series anomaly detection
method based on Spectral Residual [
46
]. Since SR does not
require training, we directly apply it in online detection.

SPOT
[
10
]: it applies Extreme Value Theory to detect
anomalies in univariate streaming data.

FluxEV
[
11
]: it augments SPOT with the capability of
identifying not only extreme values but also a range of
abnormal patterns.

Donut
[
15
]: it utilizes a variational auto-encoder (VAE) as
the backbone to serve as a reconstruction-based method for
univariate time series anomaly detection.

OmniAnomaly (OA)
[
19
]: it employs VAE to explicitly
model the dependencies among variates in a stochastic
manner for multivariate time series anomaly detection.

AnomalyTransformer (AT)
[
38
]: it adapts the Transformer
as a reconstruction-based anomaly detection model by incor-
porating an anomaly-attention mechanism and an associa-
tion discrepancy analysis method. We transform the univari-
ate anomaly score into a multivariate result for comparison.

TranAD
[
37
]: it incorporates score-based self-conditioning
adversarial training into Transformer encoder-decoder archi-
tecture for multivariate time series anomaly detection.

GDN
[
7
]: it is a GNN-based method that explicitly learns
a global static graph structure to indicate the correlations
among variates through embedding techniques.

ESG
[
32
]: it is a foresting model based on a dynamic GNN
that learns a dynamic graph structure for variates. We adapt
it for anomaly detection by employing single-step prediction
errors.

TimesNet
[
23
]: it is the state-of-the-art foundation model
for various tasks in time series analysis, including anomaly
detection tasks. It introduces a novel approach that applies
convolutions to time series by transforming them into 2D
space.
For all the baselines in our experiments, we use the imple-
mentations provided by the authors or from well-established
repositories on GitHub. We set the length of the input sequence
to be consistent with that used in AERO. The parameter
settings for each baseline are set according to the specications
detailed in their respective paper or adjusted to the optimum.
Given that different anomaly detection methods may adopt
varying criteria for selecting anomaly thresholds, we imple-
ment the POT method across all the methods to ensure a fair
2
Codes and datasets are available at: https://github.com/XinliHao/AERO
comparison. In POT, we set the initial threshold
l ev el
= 0
:
99
and desired probability
q
= 0
:
001
uniformly for all methods
on all datasets.
For the model training, we use the Adam optimizer with
an initial learning rate of 0.001. We set the length of the
sliding window
W
as 200 and the short window size
!
as
60. We set the number of layers and the number of heads
in the Transformer as 1 and 4, respectively. The maximum
number of epochs for training is set to be 100 and determined
by the early stop mechanism with
patience
= 5
.
C. Evaluation Metrics
We use precision (Prec), recall, and F1-Score (F1) over
the test datasets to evaluate the performance of all the com-
pared methods:
P r ec
=
T P
T P
+
F P
; R ecal l
=
T P
T P
+
F N
; F
1 =
2

P r ec

R ecal l
P r ec
+
R ecal l
, where
T P
,
T N
,
F P
, and
F N
are the numbers
of true positives, true negatives, false positives, and false
negatives respectively.
In line with the previous studies [
15
], [
19
], [
37
], we adopt
a point-ad just strategy to calculate the performance of these
metrics. This strategy is particularly applied in cases where
alarms are preferred to be segment-level.
D. Overall Performance (RQ1)
1) Results for Synthetic Datasets:
Table
 II
 presents the
performance of precision, recall, and F1-score for all the
compared methods on the three synthetic datasets. Based on
the results, we can make several observations.
First, it is observed that most methods designed for univari-
ate time series anomaly detection, except for SR, show the
most comp etitive performance on the SyntheticHigh dataset.
However, these methods exhibit less effective results on the
SyntheticLow dataset, apart from SPOT and TM. This trend
is in contrast to the performance of AERO, primarily due
to the limited capability of these methods in recognizing
concurrent noise. Therefore, their performance is more sus-
ceptible to anomaly-to-noise ratio (A/N). Specically, a higher
A/N ratio, indicating less concurrent noise, corresponds to
improved performance. Besides, among all the univariate time
series methods, SR achieves the best precision and F1 score,
while SPOT excels in recall. This is because SR tends to
make conservative predictions regarding potential anomalies,
in contrast to the more aggressive strategy applied in SPOT.
Second, for methods designed for multivariate time series
anomaly detection, their performance is heavily inuenced
by their capability to model the specic properties of as-
tronomical observations. Specically, TimesNet, which forms
the foundation model for time series analysis, serves as the
best-performing baseline model in terms of average F1 score
due to its robust architecture suited for a wide range of time
series data. GNN-based methodologies, particularly GDN and
ESG, closely follow in performance. Their performance results
from the capability to model explicit variate correlations. In
contrast, OmniAnomaly, AnomalyTransFormer, and TranAD
exhibit comparatively weak due to their limited ability to
accurately model concurrent noise.
TABLE II
R
E S U LT S O N T H E S Y N T H E T I C D ATA S E T S I N T E R M S O F
P
R E C I S I O N
,
R
E C A L L
,
A N D
F 1 -
S C O R E
( % ) .
Method
SyntheticMiddle SyntheticHigh SyntheticLow
Prec Recall F1
Prec Recall F1
Prec Recall F1
TM
6.08 28.98 10.06
11.64 39.13 17.94
10.19 49.38 16.90
SR
73.92 79.71 76.71
84.23 67.39 74.88
72.18 50.62 59.51
SPOT
26.74 100.0 42.20
30.91 100.0 47.23
28.05 100.0 43.81
FluxEV
57.40 55.07 56.21
81.36 84.78 83.04
61.16 49.38 54.64
Donut
61.40 50.72 55.56
78.72 53.62 63.79
43.03 25.93 32.36
OA
20.37 34.78 25.70
26.86 28.26 27.54
44.54 38.27 41.17
AT
29.76 14.49 19.49
90.55 50.00 64.43
14.79 24.69 18.50
TranAD
31.03 100.0 47.36
54.16 100.0 70.26
35.68 100.0 52.60
GDN
89.58 79.71 84.36
86.03 50.00 63.24
87.93 62.96 73.38
ESG
79.55 71.01 75.04
85.80 63.04 72.68
69.02 50.62 58.40
TimesNet
83.33 71.01 76.68
88.58 100.0 93.94
86.54 100.0 92.78
AERO
90.79 100.0 95.17
90.67 100.0 95.10
92.68 100.0 96.20
TABLE III
R
E S U LT S O N T H E R E A L
-
W O R L D D ATA S E T S I N T E R M S O F
P
R E C I S I O N
,
R
E C A L L
,
A N D
F 1 -
S C O R E
( % ) .
Method
AstrosetMiddle AstrosetHigh AstrosetLow
Prec Recall F1
Prec Recall F1
Prec Recall F1
TM
8.03 22.22 11.79
62.06 55.56 58.63
14.05 50.00 21.94
SR
76.21 100.0 86.50
74.20 100.0 85.19
82.96 91.67 87.09
SPOT
38.43 100.0 55.52
28.11 100.0 43.89
29.18
100.0
45.18
FluxEV
35.65 22.23 27.38
69.00 100.0 81.66
65.79 78.57 71.61
Donut
35.27 22.23 27.27
70.23 100.0 82.51
81.08 66.18 72.87
OA
41.93 22.23 29.05
64.10 55.56 59.52
86.37 75.00 80.28
AT
68.97 77.78 73.11
55.89 44.44 49.51
55.76 25.00 34.52
TranAD
06.47 22.23 10.03
11.61 44.44 18.42
41.61 92.86 57.47
GDN
79.72 100.0 88.71
64.94 55.56 59.88
69.20 33.33 44.99
ESG
40.24 22.23 28.63
57.47 55.56 56.50
68.18 42.86 52.63
TimesNet
41.15 22.23 28.86
68.09 55.56 61.19
85.54 91.67 88.50
AERO
80.72 100.0 89.33
75.36 100.0 85.95
89.00
91.67
90.31
Third, AERO exhibits enhanced performance with the de-
crease of anomaly-to-noise ratio (i.e., more concurrent noise
present in the SyntheticLow dataset). This superior perfor-
mance can be attributed to the design of AERO, which is
specically tailored to tackle concurrent noise and effectively
reduce the number of false positives. As a result, the precision
of AERO is greatly enhanced in scenarios of a larger ratio of
concurrent noise, like the cases encountered in astronomical
observations. As the level of concurrent noise diminishes,
the relative advantage of AERO gradually diminishes. More
importantly, AERO outperforms the baselines in terms of
all metrics on all three simulation datasets. Besides, AERO
achieves the highest average F1-score, with an improvement
of up to 8.76% over the second-best performing baseline.
2) Results for Real-world Datasets:
Table
 III
 presents the
performance of precision, recall, and F1-score for all the
compared methods on the three real-world datasets. Based on
the results, we can make several observations.
Among all baselines, SR achieves the best overall perfor-
mance. The adaptability of the spectral residual approach in
SR makes it effective for different anomaly types in real-world
datasets. Template Matching performs the worst due to the lim-
itations of pre-dened and xed templates. Consistent with its
performance on synthetic datasets, SPOT achieves the highest
recall but quite low precision. This observation again demon-
strates its tendency to predict a higher number of anomalies,
leading to numerous false alarms. FluxEV, an improvement of
SPOT, shows a more balanced precision and recall. Building
on the VAE model, Dount and OmniAnomaly produce similar
results. However, their performance drops signicantly on the
AstrosetMiddle dataset, as these methods struggle to capture
anomalies consisting of long continuous segments. TranAD,
facing similar constraints and being more sensitive to minor
uctuations, exhibits much worse performance compared to
AnomalyTransformer in real-world datasets, in contrast to the
results on synthetic datasets. Moreover, GDN demonstrates
capability in id entifying anomalies of long segments on the
AstrosetMiddle dataset. Conversely, TimesNet is effective at
detecting anomalies of relatively short time spans on the
AstrosetLow dataset.
AERO achieves the best result on the AstrosetsLow dataset
and the worst result on the AstrosetsHigh dataset for its own
performance. This pattern aligns with the trends observed in
the synthetic datasets, particularly in relation to the anomaly-
to-noise ratio (A/N). Given AERO's effectiveness in modeling
concurrent noise, its strengths become more pronounced in
scenarios where the A/N ratio is lower. Furthermore, In
comparison with the baselines, AERO surpasses them on all
three real-world datasets in most metrics, except for the recall
metric on the AstrosetsLow dataset. The most notable strength
of AERO lies in its superior precision that exceeds all the
baselines. It achieves an average improvement of 5.02% over
the best baseline. This advantage in precision also contributes
to an enhancement in F1-score, with an improvement of
up to 2.63%. The experimental results indicate that AERO
is effective at dealing with complex real-world scenarios in
astronomical observations.
E. Ablation Study (RQ2)
To examine the contributions of various components within
our method, we conduct experiments by selectively removing
different components to observe the impact on the model
performance. Specically, we implement seven variants of
the original model to validate the effectiveness of these
components. These variants are divided into two categories:
three that modify the temporal reconstruction module, and
four that modify the concurrent noise reconstruction module.
Each model variant is tested on one synthetic dataset and two
real-world datasets. We introduce the details of these model
variants below:
1) Effect of Temporal Reconstruction Module
i
w/o temporal:
it removes the temporal reconstruction
module, retaining only the concurrent noise reconstruction
module in the framework.
ii
w/o univariate input:
the input to the temporal reconstruc-
tion module is changed from univariate time series directly
to multivariate time series. This adjustment is intended
to demonstrate the effectiveness of modeling each variate
independently in this module.
TABLE IV
R
E S U LT S F O R
A
B L AT I O N
S
T U D Y
(
A S
% ) .
SyntheticMiddle AstrosetMiddle AstrosetLow
Prec Recall F1
Prec Recall F1
Prec Recall F1
AERO
90.79 100.0 95.17
80.72 100.0 89.33
89.00 91.67 90.31
1) i
43.75 20.29 27.72
70.21 77.78 73.80
84.43 75.00 79.44
1) ii
62.50 28.99 39.60
39.30 22.22 28.39
87.04 75.00 80.57
1) iii
59.52 36.23 45.05
76.27 100.0 86.54
83.33 57.14 67.80
2) i
88.69 100.0 94.00
77.12 100.0 87.08
29.59 08.33 13.00
2) ii
80.80 100.0 89.38
76.34 100.0 86.58
87.04 75.00 80.57
2) iii
74.70 71.01 72.81
74.07 100.0 85.11
86.20 91.66 88.85
2) iv
83.54 100.0 91.03
73.49 100.0 84.79
39.99 58.33 47.45
iii
w/o short window:
it removes the input from the short
window of length
!
in the temporal reconstruction module.
2) Effect of Concurrent Noise Reconstruction Module
i.
w/o concurrent noise
: it removes the concurrent noise
reconstruction module while maintaining only the temporal
reconstruction module.
ii.
w/o concurrent noise & univariate input
: it removes the
concurrent noise reconstruction module and changes the in-
put to the temporal reconstruction module as a multivariate
time series.
iii.
w/o window-wise graph (static)
: it applies a static com-
plete graph to model variate correlations rather than the
window-wise graph structure learning technique.
iv.
w/o window-wise graph (dynamic)
: it uses a dynamic
graph structure rather than a window-wise graph structure
learning technique. The dynamic graph is learned based on
ESG [
32
] to contain the output of its evolving graph layer.
The results for different model variants are presented in
Table
 IV
. Based on the results, we can observe that the removal
of different components from the framework leads to the
decline of all the metrics. This demonstrates the contributions
of each component to the model performance. Notably, the
impact of specic components varies across different datasets.
For example, the temporal reconstruction module serves as
the most inuential factor in the performance of the Synthet-
icMiddle dataset. For the AstrosetsMiddle and AstrosetsLow
datasets, the pivotal roles shift to the univariate input and the
concurrent noise reconstruction module.
On average, three model variants that either replace the
univariate input with the multivariate one (w/o univariate
input), omit the temporal construction module (w/o tempo-
ral) or remove the concurrent noise reconstruction module
(w/o concurrent noise), produce the most serious effects on
descending order. Specically, these modications result in
a substantial decrease in F1-score by 45.94%, 34.15%, and
29.38% respectively. It is interesting to observe cases where
the performance completely collapses when these critical prop-
erties are not well tackled due to the above components. This
nding highlights the importance of simultaneously addressing
both variate independence and the concurrent noise properties,
which are unique and pivotal in the context of astronomical
observations.
Moreover, compared to the proposed window-wise graph
structure learning technique, the adoption of a static graph
or the implementation of dynamic graph structures learning
leads to a decrease in F1-score by 10.20% and 18.76%
respectively. This result demonstrates that while all three
methods aim to address concurrent noise, window-wise graph
structure learning emerges as the most effective approach. The
superior performance is attributed to its more reasonable prior
assumption, which effectively models the characteristics of
spatial and temporal randomness inherent in concurrent noise.
F. Model Efciency and Scalability (RQ3)
We conduct further experiments to evaluate the model
efciency in terms of training and testing time for all the
compared methods. Note that SR is excluded from the analysis
since this method does not involve the learning process. The
results on the SyntheticMiddle dataset are reported in Fig.
 6
,
and similar trends can be observed on other datasets. For
the training stage, it can be observed that OmniAnomaly
requires the longest training time per epoch as it utilizes GRU
which sequentially processes the data points at each step,
whereas GDN is the most time-efcient due to the efciency
of the GNN model utilized in this method. The proposed
AERO model, while not the fastest, demonstrated comparable
efciency to these models. Despite its calculation of a distinct
graph structure for every sliding window, the number of
parameters remains modest to be time-efcient for training.
In the testing phase, the trends are similar to those of the
training stage: GDN is still the most efcient. Notably, AERO
demonstrates competitive efciency in the testing phase as
well. These results indicate that the efciency of our proposed
AERO is competitive in both the training and testing stages. In
this case, AERO can be deployed in online anomaly detection
to satisfy the real-time requirement at a relatively low training
cost. In addition, AERO strikes a balance between runtime
efciency and anomaly detection performance, making it a
practical choice for real-world applications requiring both
speed and accuracy.
To study the scalability of AERO, we rst analyze its
computational complexity. Given the size of long window
W
, the size of short window
!
, the number of stars
N
,
the dimension of hidden state of Transformer
d
m
, the time
complexity is
O
(
W
2
d
m
+
N !
2
)
, which remains the same
magnitude as compared to other methods.
To evalu ate its practical applicability, we generated a series
of datasets with star numbers ranging from 24 to 960 and
tested the GPU memory usage and inference time. Since in
practical scenarios, the number of stars in observed images
typically does not exceed several hundred (500), we excluded
extreme cases such as numbers over 1000. The results for GPU
memory usage and inference time are presented in Fig.
 7
.
For GPU memory usage, we observe a linear increase in
AERO, marked by a relatively modest growth rate compared to
other baselines, whereas TranAD and ESG demand the highest
usage. For inference time, ESG and SR exhibit a signicant
increase than AERO, while the increases for other baselines
are not signicant since they do not compute dynamic correla-
tion matrices. Although AERO may not demonstrate the most
superior scalability, it achieves comparable memory usage and
inference time while exhibiting the highest effectiveness. Thus,
AERO meets the requirements of practical applications in the
project of scientic discovery.
Fig. 6. Results for model efciency
Fig. 7. Results for model scalability
G. Model Analysis (RQ4)
Visualization of the window-wise graph structure.
To
further validate the effectiveness of the window-wise graph
structure learning technique in capturing concurrent noise, we
perform qualitative analysis to visualize the learned graph
structure and the ground truth graph constructed based on
concurrent noise occurrences in Fig.
 8
. The yellow dots (i.e.,
edge weights equal to 1) in Fig.
 8
(d) represent instances of
concurrent noise that affects multiple stars. It is worth noting
that these yellow dots include all instances of concurrent noise
throughout the entire time series, with each part of noise
not necessarily occurring simultaneously. Fig.
 8
(a)-(c) depict
samples from the learned window-wise graphs, extracted at
different timestamps and arranged in temporal order. We
can observe that the module accurately captures instances of
concurrent noise within specic time periods. For example,
Fig.
 8
(a) highlights concurrent noise affecting stars 1Œ4 and
6Œ9 during early timestamps. Fig.
 8
(b) captures concurrent
noise affecting stars 15Œ17 and 21Œ23 as time progresses.
Fig.
 8
(c) also aligns with concurrent noise patterns shown in
Fig.
 8
(d). These results illustrate that the learned windows-
wise graph structures effectively capture the actual occurrences
of concurrent noise across different timestamps. This ability
to accurately capture the true dynamics of concurrent noise in
the data validates that window-wise graph structure learning is
effective at handling the property in astronomical observations.
Visualization of reconstruction errors.
To analyze how
each module affects the nal anomaly score, we visualize the
reconstruction errors
Y

^
Y
1
from the temporal reconstruction
module, together with the nal reconstruction error
Y

^
Y
1

^
Y
2
Fig. 8. Visualization of graph structure. (a)-(c) are a series of window-wise
graphs from AERO before removing self-loops. They are arranged in temporal
order. (d) is the ground truth instances of concurrent noise within the entire
time series.
on several stars in Fig.
 9
. Among them, star0 and star2 display
two true anomalies, while concurrent noise occurs on star1 and
star3 at the same time.
We can observe that although true anomalies can be suc-
cessfully detected in the temporal reconstruction module,
the segments of concurrent noise are mistakenly classied
as anomalies (as indicated by the blue curves surpassing
the anomaly threshold), thus leading to false positives. This
observation suggests that the temporal reconstruction module,
in isolation, is insufcient for addressing concurrent noise
without considering the correlations among stars. However,
with the incorporation of the concurrent noise reconstruction
module, the errors corresponding to these segments are sig-
nicantly reduced. Besides, this module is capable of enlarg-
ing the reconstruction errors associated with true anomalies.
Therefore, the combination of these two modules proves to be
both reasonable and effective for this task.
Fig. 9. Visualization of reconstruction errors. star 0 and star 2 display
true anomalies at different timestamps. Concurrent noise occurs on star 1
and star 3 at the same time. Concurrent noise cannot be captured by the
temporal reconstruction module but can be ltered out by the concurrent noise
reconstruction module.
H. Parameter Sensitivity Analysis (RQ5)
We study how the value of short window size inuences the
efciency and effectiveness of AERO on the six datasets. The
results are presented in the upper part of Fig.
 10
. Regarding
model efciency, a general observation is that an increase in
short window size corresponds to longer training and testing
times (Fig.
 10
(a) and (b)). Regarding model effectiveness,
while the trends are not uniformly consistent, it is noted
Fig. 10. Parameter sensitivity analysis in AERO.
that the optimal F1 scores across all datasets are achieved
with a short window size of 60 (Fig.
 10
(c)). We infer this
phenomenon can be attributed to the limitations of both
excessively short and long windows: shorter windows may fail
to detect smaller anomalies, whereas longer windows might
not adequately represent local contextual information. Based
on the results, a short window size of 60 achieves a good
balance between achieving a high F1 score and maintaining
reasonable training and testing time. Consequently, this short
window size is selected in our experiments. We further study
the sensitivity of the other 3 parameters: the head numbers,
the number of encoder layers, and the long window size, as
depicted in Fig.
 10
(d), (e) and (f), respectively. For the head
numbers, the performance is relatively steady under different
head numbers. The optimal performance is achieved at 4 in
most cases, so we use it in other experiments considering
both performance and model complexity. For the number of
encoder layers, it can be seen that AERO achieves the best
performance with a single encoder layer across all datasets.
Therefore, we use only one layer of the encoder, which also
makes our model parameter-efcient. For the long window
size, the model achieves the best performance at 200 for all
datasets, and we set it as the default conguration.
V. C
O N C L U S I O N
We propose a two-stage anomaly detection model AERO
for tackling unique characteristics of variate independence
and concurrent noise, in astronomical observations. AERO
consists of two modules: the temporal reconstruction module
and the concurrent noise reconstruction module. First, AERO
uses a Transformer-based module to learn the normal temporal
patterns on each variate in consonance with the characteristic
of variate independence. Moreover, it devises a novel window-
wise graph learning mechanism to equip GNN with the
capacity to model random concurrent noise. The extensive
experiments on both synthetic datasets and real-world datasets
demonstrate the superiority of our method. In the future, we
plan to utilize more scalable and efcient Transformer and
GNN variants to model time-series data in more domains.
A
C K N O W L E D G E M E N T
This work is supported by the National Natural Science
Foundation of China (Grant No: 62172423, U1931133), and
the SVOM project, a mission in the Strategic Priority Program
on Space Science of CAS.
R
E F E R E N C E S
[1]
 C. Yang, Z. Du, X. Meng, X. Zhang, X. Hao, and D. A. Bader, ﬁAnomaly
detection in catalog streams,ﬂ
IEEE Transactions on Big Data
, vol. 9,
no. 1, pp. 294Œ311, 2023.
[2]
 S. Han and S. S. Woo, ﬁLearning sparse latent graph representations
for anomaly detection in multivariate time series,ﬂ in
Proceedings of
the 28th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining
, ser. KDD '22, 2022, p. 2977Œ2986.
[3]
 Z. Li, Y. Zhao, J. Han, Y. Su, R. Jiao, X. Wen, and D. Pei, ﬁMultivariate
time series anomaly detection and interpretation using hierarchical inter-
metric and temporal embedding,ﬂ in
Proceedings of the 27th ACM
SIGKDD Conference on Knowledge Discovery & Data Mining
, ser.
KDD '21, 2021, p. 3220Œ3230.
[4]
 P. Boniol, J. Paparizzos, and T. Palpanas, ﬁNew trends in time series
anomaly detection,ﬂ in
International Conference on Extending Database
Technology
, 2023.
[5]
 Z. Chen, D. Chen, X. Zhang, Z. Yuan, and X. Cheng, ﬁLearning
graph structures with transformer for multivariate time-series anomaly
detection in iot,ﬂ
IEEE Internet of Things Journal
, vol. 9, no. 12, pp.
9179Œ9189, 2022.
[6]
 C. Zhang, D. Song, Y. Chen, X. Feng, C. Lumezanu, W. Cheng, J. Ni,
B. Zong, H. Chen, and N. V. Chawla, ﬁA deep neural network for
unsupervised anomaly detection and diagnosis in multivariate time series
data,ﬂ in
Proceedings of the Thirty-Third AAAI Conference on Articial
Intelligence
, ser. AAAI'19, 2019.
[7]
 A. Deng and B. Hooi, ﬁGraph neural network-based anomaly detection
in multivariate time series,ﬂ in
Proceedings of the AAAI conference on
articial intelligence
, vol. 35, no. 5, 2021, pp. 4027Œ4035.
[8]
 Y. Cui, K. Zheng, D. Cui, J. Xie, L. Deng, F. Huang, and X. Zhou,
ﬁMetro: A generic graph neural network framework for multivariate time
series forecasting,ﬂ
Proc. VLDB Endow.
, vol. 15, no. 2, p. 224Œ236, oct
2021.
[9]
 Y. Wu, M. Gu, L. Wang, Y. Lin, F. Wang, and H. Yang, ﬁEvent2graph:
Event-driven bipartite graph for multivariate time-series anomaly
detection,ﬂ
ArXiv preprint
, vol. abs/2108.06783, 2021. [Online].
Available:
 https://arxiv.org/abs/2108.06783
[10]
 A. Siffer, P.-A. Fouque, A. Termier, and C. Largouet, ﬁAnomaly detec-
tion in streams with extreme value theory,ﬂ in
Proceedings of the 23rd
ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining
, ser. KDD '17, 2017, p. 1067Œ1075.
[11]
 J. Li, S. Di, Y. Shen, and L. Chen, ﬁFluxev: A fast and effective unsu-
pervised framework for time-series anomaly detection,ﬂ in
Proceedings
of the 14th ACM International Conference on Web Search and Data
Mining
, ser. WSDM '21, 2021, p. 824Œ832.
[12]
 H. Ren, B. Xu, Y. Wang, C. Yi, C. Huang, X. Kou, T. Xing, M. Yang,
J. Tong, and Q. Zhang, ﬁTime-series anomaly detection service at
microsoft,ﬂ in
Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining
, ser. KDD '19,
2019, p. 3009Œ3017.
[13]
 P. Malhotra, L. Vig, G. Shroff, P. Agarwal
et al.
, ﬁLong short term
memory networks for anomaly detection in time series.ﬂ in
Esann
, vol.
2015, 2015, p. 89.
[14]
 S. Hochreiter and J. Schmidhuber, ﬁLong short-term memory,ﬂ
Neural
Computation
, vol. 9, no. 8, pp. 1735Œ1780, 1997.
[15]
 H. Xu, W. Chen, N. Zhao, Z. Li, J. Bu, Z. Li, Y. Liu, Y. Zhao,
D. Pei, Y. Feng, J. Chen, Z. Wang, and H. Qiao, ﬁUnsupervised
anomaly detection via variational auto-encoder for seasonal kpis in web
applications,ﬂ in
Proceedings of the 2018 World Wide Web Conference
,
ser. WWW '18, 2018, p. 187Œ196.
[16]
 J. An and S. Cho, ﬁVariational autoencoder based anomaly detection
using reconstruction probability,ﬂ
Special lecture on IE
, vol. 2, no. 1,
pp. 1Œ18, 2015.
[17]
 S. Lin, R. Clark, R. Birke, S. Sch
¨
onborn, N. Trigoni, and S. Roberts,
ﬁAnomaly detection for time series using vae-lstm hybrid model,ﬂ in
ICASSP 2020 - 2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)
, 2020, pp. 4322Œ4326.
[18]
 K. Hundman, V. Constantinou, C. Laporte, I. Colwell, and T. Soder-
strom, ﬁDetecting spacecraft anomalies using lstms and nonparametric
dynamic thresholding,ﬂ in
Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining
, ser.
KDD '18, 2018, p. 387Œ395.
[19]
 Y. Su, Y. Zhao, C. Niu, R. Liu, W. Sun, and D. Pei, ﬁRobust anomaly
detection for multivariate time series through stochastic recurrent neural
network,ﬂ in
Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining
, ser. KDD '19,
2019, p. 2828Œ2837.
[20]
 T. Kieu, B. Yang, C. Guo, R.-G. Cirstea, Y. Zhao, Y. Song, and C. S.
Jensen, ﬁAnomaly detection in time series with robust variational quasi-
recurrent autoencoders,ﬂ in
2022 IEEE 38th International Conference
on Data Engineering (ICDE)
, 2022, pp. 1342Œ1354.
[21]
 D. Li, D. Chen, B. Jin, L. Shi, J. Goh, and S.-K. Ng, ﬁMad-gan:
Multivariate anomaly detection for time series data with generative ad-
versarial networks,ﬂ in
Articial Neural Networks and Machine Learning
Œ ICANN 2019: Text and Time Series: 28th International Conference on
Articial Neural Networks, Munich, Germany, September 17Œ19, 2019,
Proceedings, Part IV
, 2019, p. 703Œ716.
[22]
 X. Chen, L. Deng, F. Huang, C. Zhang, Z. Zhang, Y. Zhao, and
K. Zheng, ﬁDaemon: Unsupervised anomaly detection and interpretation
for multivariate time series,ﬂ in
2021 IEEE 37th International Confer-
ence on Data Engineering (ICDE)
, 2021, pp. 2225Œ2230.
[23]
 H. Wu, T. Hu, Y. Liu, H. Zhou, J. Wang, and M. Long, ﬁTimesnet:
Temporal 2d-variation modeling for general time series analysis,ﬂ
arXiv
preprint arXiv:2210.02186
, 2022.
[24]
 F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
ﬁThe graph neural network model,ﬂ
IEEE Transactions on Neural
Networks
, vol. 20, no. 1, pp. 61Œ80, 2009.
[25]
 Y. Wu, H.-N. Dai, and H. Tang, ﬁGraph neural networks for anomaly
detection in industrial internet of things,ﬂ
IEEE Internet of Things
Journal
, vol. 9, no. 12, pp. 9214Œ9231, 2022.
[26]
 H. Zhao, Y. Wang, J. Duan, C. Huang, D. Cao, Y. Tong, B. Xu, J. Bai,
J. Tong, and Q. Zhang, ﬁMultivariate time-series anomaly detection via
graph attention network,ﬂ in
2020 IEEE International Conference on
Data Mining (ICDM)
, 2020, pp. 841Œ850.
[27]
 J. Zhan, S. Wang, X. Ma, C. Wu, C. Yang, D. Zeng, and S. Wang, ﬁStgat-
mad : Spatial-temporal graph attention network for multivariate time
series anomaly detection,ﬂ in
ICASSP 2022 - 2022 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP)
, 2022,
pp. 3568Œ3572.
[28]
 Z. Wu, S. Pan, G. Long, J. Jiang, X. Chang, and C. Zhang, ﬁCon-
necting the dots: Multivariate time series forecasting with graph neural
networks,ﬂ in
Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining
, ser. KDD '20,
2020, p. 753Œ763.
[29]
 H. Yu, T. Li, W. Yu, J. Li, Y. Huang, L. Wang, and A. Liu, ﬁRegularized
graph structure learning with semantic knowledge for multi-variates
time-series forecasting,ﬂ
arXiv preprint arXiv:2210.06126
, 2022.
[30]
 X. Chen, Q. Qiu, C. Li, and K. Xie, ﬁGraphad: A graph neural
network for entity-wise multivariate time-series anomaly detection,ﬂ
in
Proceedings of the 45th International ACM SIGIR Conference on
Research and Development in Information Retrieval
, ser. SIGIR '22,
2022, p. 2297Œ2302.
[31]
 J. Chen, Y. Yang, T. Yu, Y. Fan, X. Mo, and C. Yang, ﬁBrainnet: Epileptic
wave detection from seeg with hierarchical graph diffusion learning,ﬂ
in
Proceedings of the 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining
, ser. KDD '22, 2022, p. 2741Œ2751.
[32]
 J. Ye, Z. Liu, B. Du, L. Sun, W. Li, Y. Fu, and H. Xiong, ﬁLearning the
evolutionary and multi-scale graph structure for multivariate time series
forecasting,ﬂ in
Proceedings of the 28th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining
, ser. KDD '22, 2022, p.
2296Œ2306.
[33]
 W. T. Ng, K. Siu, A. C. Cheung, and M. K. Ng, ﬁExpressing multivariate
time series as graphs with time series attention transformer,ﬂ
arXiv
preprint arXiv:2208.09300
, 2022.
[34]
 Y. Fang, K. Ren, C. Shan, Y. Shen, Y. Li, W. Zhang, Y. Yu, and
D. Li, ﬁLearning decomposed spatial relations for multi-variate time-
series modeling,ﬂ in
Proceedings of the Thirty-Seventh AAAI Conference
on Articial Intelligence
, ser. AAAI'23, 2023.
[35]
 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, and I. Polosukhin, ﬁAttention is all you need,ﬂ in
Proceedings
of the 31st International Conference on Neural Information Processing
Systems
, ser. NIPS'17, 2017, p. 6000Œ6010.
[36]
 Q. Wen, T. Zhou, C. Zhang, W. Chen, Z. Ma, J. Yan, and L. Sun, ﬁTrans-
formers in time series: A survey,ﬂ
arXiv preprint arXiv:2202.07125
,
2022.
[37]
 S. Tuli, G. Casale, and N. R. Jennings, ﬁTranad: Deep transformer
networks for anomaly detection in multivariate time series data,ﬂ
Proc.
VLDB Endow.
, vol. 15, no. 6, p. 1201Œ1214, feb 2022.
[38]
 J. Xu, H. Wu, J. Wang, and M. Long, ﬁAnomaly transformer: Time
series anomaly detection with association discrepancy,ﬂ
arXiv preprint
arXiv:2110.02642
, 2021.
[39]
 J. D. M.-W. C. Kenton and L. K. Toutanova, ﬁBert: Pre-training of deep
bidirectional transformers for language understanding,ﬂ in
Proceedings
of naacL-HLT
, vol. 1, 2019, p. 2.
[40]
 L. Dong, S. Xu, and B. Xu, ﬁSpeech-transformer: A no-recurrence
sequence-to-sequence model for speech recognition,ﬂ in
2018 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP)
, 2018, pp. 5884Œ5888.
[41]
 X. Hu, L.-X. Zhang, L. Gao, W. Dai, X. Han, Y.-K. Lai, and Y. Chen,
ﬁGlim-net: Chronic glaucoma forecast transformer for irregularly sam-
pled sequential fundus images,ﬂ
IEEE Transactions on Medical Imaging
,
vol. 42, no. 6, pp. 1875Œ1884, 2023.
[42]
 S. N. Shukla and B. M. Marlin, ﬁMulti-time attention networks for
irregularly sampled time series,ﬂ
arXiv preprint arXiv:2101.10318
, 2021.
[43]
 J. R. Davenport, S. L. Hawley, L. Hebb, J. P. Wisniewski, A. F. Kowalski,
E. C. Johnson, M. Malatesta, J. Peraza, M. Keil, S. M. Silverberg
et al.
,
ﬁKepler ares. ii. the temporal morphology of white-light ares on gj
1243,ﬂ
The Astrophysical Journal
, vol. 797, no. 2, p. 122, 2014.
[44]
 G.-W. Li, C. Wu, G.-P. Zhou, C. Yang, H.-L. Li, J. Chen, L.-P. Xin,
J. Wang, H. Haerken, C.-H. Ma
et al.
, ﬁMagnetic activity and parameters
of 43 are stars in the gwac archive,ﬂ
Research in Astronomy and
Astrophysics
, vol. 23, no. 1, p. 015016, 2023.
[45]
 Z. Duan, C. Yang, X. Meng, Y. Du, J. Qiu, X. Ma, Z. Du, X. Zhang,
B. Niu, and C. Wu, ﬁScidetector: Scientic event discovery by track-
ing variable source data streaming,ﬂ in
2019 IEEE 35th International
Conference on Data Engineering (ICDE)
, 2019, pp. 2040Œ2043.
[46]
 X. Hou and L. Zhang, ﬁSaliency detection: A spectral residual approach,ﬂ
in
2007 IEEE Conference on Computer Vision and Pattern Recognition
,
2007, pp. 1Œ8.
