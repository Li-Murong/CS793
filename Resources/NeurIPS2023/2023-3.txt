Newton–Cotes Graph Neural Networks:
On the Time Evolution of Dynamic Systems
Lingbing Guo1,2,3∗, Weiqing Wang4∗, Zhuo Chen1,2,3 , Ningyu Zhang1,2 ,
Zequn Sun5 , Yixuan Lai1,2,3 , Qiang Zhang1,3†, and Huajun Chen1,2,3†
1
College of Computer Science and Technology, Zhejiang University
2
Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph
3
ZJU-Hangzhou Global Scientific and Technological Innovation Center
4
Department of Data Science & AI, Monash University
5
State Key Laboratory for Novel Software Technology, Nanjing University

Abstract
Reasoning system dynamics is one of the most important analytical approaches
for many scientific studies. With the initial state of a system as input, the recent
graph neural networks (GNNs)-based methods are capable of predicting the future
state distant in time with high accuracy. Although these methods have diverse
designs in modeling the coordinates and interacting forces of the system, we
show that they actually share a common paradigm that learns the integration
of the velocity over the interval between the initial and terminal coordinates.
However, their integrand is constant w.r.t. time. Inspired by this observation,
we propose a new approach to predict the integration based on several velocity
estimations with Newton–Cotes formulas and prove its effectiveness theoretically.
Extensive experiments on several benchmarks empirically demonstrate consistent
and significant improvement compared with the state-of-the-art methods.

1

Introduction

Reasoning the time evolution of dynamic systems has been a long-term challenge for hundreds of years
[1, 2]. Despite that the advances in computer manufacturing nowadays make it possible to simulate
long trajectories of complicated systems constrained by multiple force laws, the computational cost
still imposes a heavy burden on the scientific communities [3–8].
Recent graph neural networks (GNNs) [9–13]-based methods provide an alternative solution to
predict the future states directly with only the initial state as input [14–20]. Take molecular dynamics
(MD) [4, 21–24] as an example, the atoms in a molecule can be regarded as nodes with different
labels, and thus can be encoded by a GNN. As the input and the target are atomic coordinates,
the learning problem becomes a complicated regression task of predicting the coordinate at each
dimension of each atom. Furthermore, some directional information (e.g., velocities or forces) are also
important features and cannot be directly leveraged especially considering the rotation and translation
of molecules in the system. Therefore, the existing works put great effort into the reservation of
physical symmetries, e.g., transformation equivariance and geometric constraints [18, 19]. The
experimental results on a variety of benchmarks also demonstrate the advantages of their methods.
Without loss of generality, the main target of the existing works is to predict the future coordinate
xT distant to the given initial coordinate x0 , with x0 and the initial velocity v0 as input. Then, the
∗
†

Equal Contribution
Correspondence to: {qiang.zhang.cs, huajunsir}@zju.edu.cn,

37th Conference on Neural Information Processing Systems (NeurIPS 2023).

predicted coordinate x̂T can be written as:
x̂T = x0 + v̂0 T.

(1)

For a complicated system comprising multiple particles, predicting the velocity term v̂0 rather than
the coordinate x̂T improves the robustness and performance. Specifically, by subtracting the initial
coordinate x0 , the learning target can be regarded as the normalized future coordinate, i.e., v̂0 =
(xT −x0 )/(T −0), which is why all state-of-the-art methods adopt this strategy [18, 19]. Nevertheless,
the current strategy still has a significant drawback from the view of numerical integration.
As illustrated in Figure 1, supposed that the actual velocity
of a particle is described by the curve v(t). To predict the
future state xT at t = T , the existing methods adopt a constant
RT
estimation approach, i.e., x̂T = x0 + v̂0 T = x0 + 0 v̂0 dt.
Evidently, the prediction error could be significant as it purely
relies on the fitness of neural models. If we alternatively choose
a simple two-step estimation (i.e., Trapezoidal rule [25]), the
prediction error may be reduced to only the blue area. In other
words, the model only needs to compensate for the blue area.

𝑣𝑣

𝑣𝑣(𝑡𝑡)

𝑣𝑣 0
0

The existing methods
The two-step estimation
The true integration over [0, 𝑇𝑇]

𝑣𝑣 𝑇𝑇
𝑇𝑇

𝑡𝑡

Figure 1: Illustration of different
estimations. The rectangle, trapezoid, and striped areas denote the
basic (used in the existing works),
two-step, and true estimations, respectively. Blue denotes the error
of two-step estimation that a model
needs to compensate for, whereas
blue + yellow denotes the error of
the existing methods.

In this paper, we propose Newton–Cotes graph neural networks
(abbr. NC) to estimate multiple velocities at different time
points and compute the integration with Newton-Cotes formulas. Newton-Cotes formulas are a series of formulas for
numerical integration in which the integrand (in our case, v(t))
are evaluated at equally spaced points. One most important
characteristic of Newton-Cotes formulas is that the integration
is computed by aggregating the values of v(t) at these points
with a group of weights {w0 , ..., wk }, where k refers to the
order of Newton–Cotes formulas and {w0 , ..., wk } is irrelevant
to the integrand v(t) and can be pre-computed by Lagrange interpolating polynomial [26].

We show that the existing works can be naturally derived to the basic version NC (k = 0) and
theoretically prove that the prediction error of this estimation as well as the learning difficulty will
continually reduce as the increase of estimation step k.
To better train a NC (k), we may also need the intermediate velocities as additional supervised
data. Although these data can be readily obtained (as the sampling frequency is much higher than
our requirement), we argue that the performance suffers slightly even if we do not use them. The
model is capable of learning to generate promising velocities to better match the true integration. By
contrast, if we do provide these data to train a NC (k), denoted by NC+ (k), we will obtain a stronger
model that not only achieves high prediction accuracy in conventional tasks, but also produces highly
reliable results of long-term consecutive predictions.
We conduct experiments on several datasets ranging from N-body systems to molecular dynamics
and human motions [27–29], with state-of-the-art methods as baselines [17–19]. The results show
that NC improves all the baseline methods with a significant margin on all types of datasets, even
without any additional training data. Particularly, the improvement for the method RF [17] is greater
than 30% on almost all datasets.

2

Related Works

In this section, we first introduce the related works using GNNs to learn system dynamics and then
discuss more complicated methods that possess the equivariance properties.
Graph Neural Networks for Reasoning Dynamics Interaction network [30] is perhaps the first
GNN model that learns to capture system dynamics. It separates the states and correlations into
two different parts and employs GNNs to learn their interactions. This progress is similar to some
simulation tools where the coordinate and velocity/force are computed and updated in an alternative
fashion. Many followers extend this idea, e.g., using hierarchical graph convolution [9, 10, 31, 32] or
auto-encoder [33–36] to encode the objects, or leveraging ordinary differential equations for energy
2

conservation [37]. The above methods usually cannot process the interactions among particles in
the original space (e.g., Euclidean space). They instead propose an auxiliary interaction graph to
compute the interactions, which is out of our main focus.
Equivariant Graph Neural Networks Recent methods considering physical symmetry and Euclidean equivariance have achieved state-of-the-art performance in modeling system dynamics [14–
19, 38]. Specifically, [14–16] force the translation equivariance, but the rotation equivariance is
overlooked. TFN [39] further leverages spherical filters to achieve rotation equivariance. SE(3) Transformer [29] proposes to use the Transformer [13] architecture to model 3D cloud points directly in
Euclidean space. EGNN [18] simplifies the equivariant graph neural network and make it applicable
in modeling system dynamics. GMN [19] proposes to consider the physical constraints (e.g., sticks
and hinges sub-structures) widely existed in systems, which achieves the state-of-the-art performance
while maintaining the same level of computational cost. SEGNN [38] extends EGNN with steerable
vectors to model the covariant information among nodes and edges. Note that, not all EGNNs are
designed to reason system dynamics, but the best-performing ones (e.g., EGNN [18] and GMN [19])
in this sub-area usually regard the velocity v as an important feature. These methods can be set as the
backbone model in NC, and the models themselves belong to the basic NC (0).
Neural Ordinary Differential Equations The neural ordinary differential equation (neural ODE)
methods [40, 41] parameterize ODEs with neural layers and solve them by numerical solvers such as
4th order Runge-Kutta. These solvers are originally designed for numerical integration. Our work
draws inspiration from numerical integration algorithms, where the integrand is known only at certain
points. The goal is to efficiently approximate the integral to desired precision. Neural ODE methods
repeatedly perform the numerical algorithms on small internals to obtain the numerical solution.
Therefore, it may be feasible to iteratively perform our method to solve problems in neural ODEs.

3

Methodology

We start from preliminaries and then take the standard EGNN [18] as an example to illustrate how the
current deep learning methods work. We show that the learning paradigm of the existing methods can
be derived to the simplest form of numerical integration. Finally, we propose NC and theoretically
prove its effectiveness in predicting future states.
3.1

Preliminaries

We suppose that a system comprises N particles p1 , p2 , ..., pN , with the velocities v1 , v2 , ..., vN and
the coordinates x1 , x2 , ..., xN as states. The particles can also carry various scalar features (e.g.,
mass and charge for atoms) which will be encoded as embeddings. We follow the corresponding
existing works [17–19] to process them and do not discuss the details in this paper.
Reasoning Dynamics Reasoning system dynamics is a classical task with a very long history [1].
One basic tool for reasoning or simulating a system is numerical integration. Given the dynamics (e.g.,
Langevin dynamics, well-used in MD) and initial information, the interaction force and acceleration
for each particle in the system can be estimated. However, the force is closely related to the real-time
coordinate, which means that one must re-calculate the system states for every very small time step
(i.e., dt) to obtain a more accurate prediction for a long time interval. For example, in MD simulation,
the time step dt is usually set to 50 or 100 fs (1 fs = 10−15 second), whereas the total simulation time
can be 1 ms (10−3 second). Furthermore, pursuing highly accurate results usually demands more
complicated dynamics, imposing heavy burdens even for super-computers. Therefore, leveraging
neural models to directly predict the future state of the system has recently gained great attention.
3.2

EGNN

Although the existing EGNN methods have great advantage in computational cost over the conventional simulation tools, the potential prediction error that a neural model needs to compensate for is
also huge. Take the standard EGNN [18] as an example, the equivariant graph convolution can be
written as follows:
m0ij = ϕe (hi , hj , ||x0i − x0j ||2 , eij ),
(2)
3

where the aggregation function ϕe takes three types of information as input: the feature embeddings
hi and hj for the input particle pi and an arbitrary particle pj , respectively; the Euclidean distance
||x0i − x0j ||2 at time t = 0; and the edge attribute eij . Then, the predicted velocity and future
coordinate can be calculated by the following equation:
v̂i0 = ϕv (hi )vi0 +

1 X 0
(xi − x0j )m0ij ,
N −1

(3)

j̸=i

x̂Ti = x0i + v̂i0 T,

(4)

where ϕv : RD → R1 is a learnable function. From the above equations, we can find that the
predicted velocity v̂i0 is also correlated with three types of information: 1) the initial velocity vi0
as a basis; 2) the pair-wise Euclidean distance ||x0i − x0j ||2 (i.e., m0ij ) to determine the amount of
force (analogous to Coulomb’s law); and the pair-wise relative coordinate difference (x0i − x0j ) to
determine the direction. For multi-layer EGNN and other EGNN models, v̂i0 is still determined by
these three types of information, which we refer interested readers to Appendix A for details.
Proposition 3.1 (Linear mapping). The existing methods learn a linear mapping from the initial
∗
velocity v0 to the average velocity vt over the interval [0, T ].
Proof. Please see Appendix B.1
∗

As the model makes prediction only based on the state and velocity at t = 0, we assume that vt
2
fluctuates around v0 and follows a normal distribution denoted by NN C(0) = (v0 , σN
C(0) ). Then,
2
the variance term σN C(0) reflects how difficult to train a neural model on data sampled from this
distribution. In other words, the larger the variance is, the worse the expected results are.
∗

Proposition 3.2 (Variance of NN C(0) ). Assume that the average velocity vt over [0, T ] follows a
normal distribution with µ = v0 , then the variance of the distribution is:
2
2
σN
C(0) = O(T )

(5)

2
Proof. According to the definition, σN
C(0) can be written as follows:

P

p (v

2
σN
C(0) =

t∗

− v0 )2

P
=

p ((v

M
T
0
0
2
p ((x − x ) − v T )

t∗

P
=

MT2

=

T − v0 T )/T )2

M
P RT
0
2
p ( 0 (v(t) − v T )dt)
MT2

(6)
,

(7)

where M ≫ N is the number of all samples. The term (v(t)−v0 T ) is a typical (degree 0) polynomial
interpolation error and can be defined as:
ϵIP(0) (t) = v(t) − v0 T = v(t) − Pk (t) k = 0

(8)

(k+1)

=

v
(ξ)
(t − t0 )(t − t1 )...(t − tk ) k = 0 = v(ξ)t,
(k + 1)!

where 0 ≤ ξ ≤ T , and the corresponding integration error is:
Z T
Z T
1
ϵNC(0) =
ϵIP(0) (t)dt = v(ξ)
tdt = v(ξ)T 2 = O(T 2 ).
2
0
0

(9)

(10)

Therefore, we obtain the final variance:
2
σN
C(0) =

M O(T 4 )
= O(T 2 ),
MT2

concluding the proof.
4

(11)

3.3

NC (k)

In comparison with the 0 degree polynomial approximation NC(0), the high order NC(k) is more
accurate and yields only a little extra computational cost.
Suppose that we now have K + 1 (usually K ≤ 8 due to the catastrophic Runge’s phenomenon [42])
points x0i , x1i , ..., xK
i equally spaced on time interval [0, T ], then the prediction for the future coordinate x̂Ti in Equation (4) can be re-written as:
K

x̂Ti = x0i +

T X k k
w v̂i ,
K

(12)

k=0

where {wk } is the coefficients of Newton-Cotes formulas and v̂ik denotes the predicted velocity at
time tk . To obtain v̂ik , k ≥ 1, we simply regard EGNN as a recurrent model [43, 44] and re-input the
last output velocity and coordinate. Some popular sequence models like LSTM [45] or Transformer
[13] may be also leveraged, which we leave to future work. Then, the equation of updating the
predicted velocity v̂ik at tk can be written as:
P
k−1
− xk−1
)mij
j
j̸=i (xi
k−1
k
v̂i = ϕv (hi )vi +
,
(13)
N −1
where vik−1 , xk−1
denote the velocity and coordinate at tk−1 , respectively. Note that, Equation (13)
i
is different from the form used in a multi-layer EGNN. The latter always uses the same input velocity
and coordinate as constant features in different layers. By contrast, we first predict the next velocity
and coordinate and then use them as input to get the new trainable velocity and coordinate. This
process is more like training a language model [13, 43, 46, 45, 47].
PK
From the interpolation theorem [26, 48], there exists a unique polynomial k=0 C k t(k) of degree at
most K that interpolates the K + 1 points, where C k is the coefficients of the polynomial. To avoid
ambiguity, we here use t(k) to denote t raised to the power of k, whereas tk to denote the k-th time
PK
point. k=0 C k t(k) thus can be constructed by the following equation:
K
X

C k t(k) = [v(t0 )] + [v(t0 ), v(t1 )](t − t0 ) + ... + [v(t0 ), ..., v(tK )](t − t0 )(t − t1 )...(t − tK ),

k

(14)
where [·] denotes the divided difference. In our case, the K + 1 points are equally spaced over the
interval [0, T ]. According to Newton-Cotes formulas, we will have:
Z T
Z TX
K
K
T X k k
(k)
Ck t dt =
v(t)dt ≈
w v .
(15)
K
0
0
k=0

k=0

k

Particularly, {w } are Newton-Cotes coefficients that are irrelevant to v(t). With additional observable points, the estimation for the integration can be much more accurate. The objective thus can be
written as follows:
K

argmin

X

{v̂k }

p

xT − x̂T =

X
p

xT − x0 −

T X k k
w v̂ .
K

(16)

k=0

2
Proposition 3.3 (Variance of σN
C(k) ). ∀k ∈ {0, 1, ...}, ϵNC(k) ≥ ϵNC(k+1) , and consequently:
2
2
2
σN
C(0) ≥ σN C(1) ≥ ... ≥ σN C(k) .

(17)

Proof. Please see Appendix B.2 for details. Briefly, we only need to prove that ϵNC(0) ≥ ϵNC(1) ,
where NC (1) is associated with Trapezoidal rule.
Proposition 3.4 (Equivariance of NC). NC possesses the equivariance property if the backbone
model M (e.g., EGNN) in NC possesses this property.
Proof. Please see Appendix B.3.
5

Algorithm 1 Newton-Cotes Graph Neural Network
1: Input: the dataset D, the backbone model M, number of steps k for NC;
2: repeat
3:
for each batched data in the training set ({X 0 , ..., X k }, {V 0 , ..., V k }) do
4:
X̂ 0 ← X 0 , V̂ 0 ← V 0 ;
5:
for i := 1 to k do
6:
X̂ i , V̂ i ← M(X̂ i−1 , V̂ i−1 );
7:
end for
8:
Compute the final prediction X̂ k by Equation 12;
9:
Compute the main prediction loss Lmain according to Equation (16);
10:
Compute the velocity regularization loss Lr according to Equation (18);
11:
Minimize Lmain , Lr ;
12:
end for
13: until the loss on the validation set converges.

3.4

NC (k) and NC+ (k)

In the previous formulation, in addition to the initial and terminal points, we also need the other
K − 1 points for supervision which yields a regularization loss:
Lr =

K
XX

∥vk − v̂k ∥.

(18)

p k=0

We denote NC (k) with the above loss by NC+ (k). Minimizing Lr is equivalent to minimizing the
difference between the true velocity and predicted velocity at each time point tk for each particle. ∥ · ∥
can be arbitrary distance measure, e.g., L2 distance. Although the K − 1 points of data can be easily
accessed in practice, we argue that a loose version (without Lr ) NC (k) can also learn promising
RT
estimation of the integration 0 v(t)dt. Specifically, we still use K + 1 points {v̂k } to calculate the
integration over [0, T ], except that the intermediate K − 1 points are unbounded. The model itself
needs to determine the proper values of {v̂k } to optimize the same objective defined in Equation 16.
In Section 4.5, we design an experiment to investigate the difference between the true velocities {vk }
and the predicted velocities {v̂k }.
3.5

Computational Complexity and Limitations

In comparison with EGNN, NC (k) does not involve additional neural layers or parameters. Intuitively,
we recurrently use the last output of EGNN to produce new predicted velocities at next time point,
the corresponding computational cost should be K times that of the original EGNN. However, our
experiment in Section 4.4 shows that the training time did not actually linearly increase w.r.t. K. One
reason may be the gradient in back-propagation is still computed only once rather than K times.
We present an implementation of NC+ in Algorithm 1. We first initialize all variables in the model
and then recurrently input the last output to the backbone model to collect a series of predicted
velocities. We then calculate the final predicted integration and plus the initial coordinate as the final
output (i.e., Equation (12)). Finally we compute the losses and update the model by back-propagation.

4

Experiment

We conducted experiments on three different tasks towards reasoning system dynamics. The source
code and datasets are available at https://github.com/zjukg/NCGNN.
4.1

Settings

We selected three state-of-the-art methods as our backbone models: RF [17], EGNN [18], and
GMN [19]. To ensure a fair comparison, we followed their best parameter settings (e.g., hidden size
and the number of layers) to construct NC. In other words, the main settings of NC and the baselines
6

Table 1: Prediction error (×10−2 ) on N-body dataset. The header of each column “p, s, h" denotes
the scenario with p isolated particles, s sticks and h hinges. Results averaged across 3 runs.
1,2,0

|Train| = 500
2,0,1
3,2,1
0,10,0

5,3,3

1,2,0

|Train| = 1500
2,0,1
3,2,1
0,10,0

5,3,3

Linear
8.23±0.00 7.55±0.00 9.76±0.00 11.36±0.00 11.62±0.00 8.22±0.00 7.55±0.00 9.76±0.00 11.36±0.00 11.62±0.00
GNN [9]
5.33±0.07 5.01±0.08 7.58±0.08 9.83±0.04 9.77±0.02 3.61±0.13 3.23±0.07 4.73±0.11 7.97±0.44 7.91±0.31
TFN [39]
11.54±0.38 9.87±0.27 11.66±0.08 13.43±0.31 12.23±0.12 5.86±0.35 4.97±0.23 8.51±0.14 11.21±0.21 10.75±0.08
SE(3)-Tr. [29] 5.54±0.06 5.14±0.03 8.95±0.04 11.42±0.01 11.59±0.01 5.02±0.03 4.68±0.05 8.39±0.02 10.82±0.03 10.85±0.02
RF [17]
NC (RF)
NC+ (RF)

3.50±0.17 3.07±0.24 5.25±0.44 7.59±0.25 7.73±0.39 2.97±0.15 2.19±0.11 3.80±0.25 5.71±0.31 5.66±0.27
2.84±0.01 2.35±0.04 4.32±0.08 6.67±0.26 7.14±0.25 2.91±0.11 1.76±0.01 3.23±0.01 5.09±0.05 5.34±0.03
2.92±0.10 2.34±0.03 4.28±0.05 7.02±0.14 7.06±0.33 2.87±0.05 1.78±0.01 3.24±0.02 5.06±0.06 5.23±0.29

EGNN [18]
2.81±0.12 2.27±0.04 4.67±0.07 4.75±0.05 4.59±0.07 2.59±0.10 1.86±0.02 2.54±0.01 2.79±0.04 3.25±0.07
NC (EGNN) 2.41±0.03 2.18±0.02 3.53±0.01 4.26±0.03 4.13±0.03 2.23±0.13 1.91±0.03 2.14±0.03 2.36±0.05 2.86±0.03
+
NC (EGNN) 2.25±0.01 2.07±0.06 2.01±0.02 3.54±0.06 3.96±0.04 2.32±0.05 1.86±0.13 2.39±0.01 2.35±0.07 2.99±0.02
GMN [19]
NC (GMN)
NC+ (GMN)

1.84±0.02 2.02±0.02 2.48±0.04 2.92±0.04 4.08±0.03 1.68±0.04 1.47±0.03 2.10±0.04 2.32±0.02 2.86±0.01
1.55±0.07 1.58±0.02 2.07±0.03 2.73±0.02 2.99±0.03 1.59±0.03 1.18±0.05 1.47±0.01 1.66±0.01 1.93±0.03
1.57±0.02 1.43±0.02 2.03±0.04 2.57±0.04 2.72±0.03 1.49±0.02 1.17±0.04 1.44±0.01 1.70±0.06 1.91±0.02

Table 2: Prediction error (×10−2 ) on MD17 dataset. Results averaged across 3 runs.
Aspirin
Raw
NC
NC+

Benzene

Malonaldehyde

EGNN

GMN

RF

EGNN

GMN

RF

EGNN

GMN

RF

EGNN

GMN

10.94±0.01
10.87±0.01
10.87±0.01

14.41±0.15
9.63±0.01
9.60±0.02

10.14±0.03
9.50±0.06
9.40±0.09

103.72±1.29
63.22±0.01
63.04±0.05

62.40±0.53
55.05±1.60
54.76±0.74

48.12±0.40
41.62±1.16
41.26±0.15

4.64±0.01
4.62±0.01
4.62±0.01

4.64±0.01
4.63±0.01
4.62±0.01

4.83±0.01
4.83±0.01
4.83±0.01

13.93±0.03
12.93±0.03
12.93±0.03

13.64±0.01
12.82±0.01
12.81±0.02

13.11±0.03
12.97±0.01
12.92±0.01

Naphthalene
Raw
NC
NC+

Ethanol

RF

Salicylic

Toluene

Uracil

RF

EGNN

GMN

RF

EGNN

GMN

RF

EGNN

GMN

RF

EGNN

GMN

0.50±0.01
0.39±0.01
0.39±0.01

0.47±0.02
0.37±0.01
0.37±0.01

0.40±0.01
0.38±0.01
0.37±0.02

1.23±0.01
1.09±0.01
1.09±0.01

1.02±0.02
0.84±0.01
0.84±0.01

0.91±0.01
0.86±0.01
0.86±0.01

10.93±0.04
10.85±0.01
10.85±0.01

11.78±0.07
10.64±0.11
10.52±0.01

10.22±0.08
10.17±0.04
10.14±0.01

0.64±0.01
0.60±0.01
0.60±0.01

0.64±0.01
0.57±0.01
0.57±0.01

0.59±0.01
0.56±0.01
0.56±0.02

were identical. The results reported in the main experiments were based on a model of NC (2) for a
balance of training cost and performance.
We reported the performance of NC w/ additional points (denoted by NC+ ) and w/o additional points
(the relaxed version, denoted by NC).
4.2

Datasets

We leveraged three datasets towards different aspects of reasoning dynamics to exhaust the performance of NC. We used the N-body simulation benchmark proposed in [19]. Compared with the
previous ones, this benchmark has more different settings, e.g., number of particles and training data.
Furthermore, it also considers some physical constraints (e.g., hinges and sticks) widely existing
in the real world. For molecular dynamics, We used MD17 [28] that consists of the molecular
dynamics simulations of eight different molecules as a scenario. We followed [19] to construct the
training/testing sets. We also considered reasoning human motion. Follow [19], the dataset was
constructed based on 23 trials in CMU Motion Capture Database [27].
As NC+ requires additional supervised data, we accordingly extracted more intermediate data points
between the input and target from each dataset. It is worth noting that this operation was tractable as
the sampling frequency of the datasets was much higher than our requirement.
4.3

Main Results

We reported the main experimental results w.r.t. three datasets in Tables 1, 2, and 3, respectively.
Overall, NC+ significantly improved the performance of all baselines on almost all datasets and
across all settings, which empirically demonstrates the generality as well as the effectiveness of the
proposed method. Remarkably, NC (without extra data) achieved slightly low results but still had
great advantages over performance in comparison with three baselines.
Specifically, The average improvement on N-body datasets was most significant, where we observe a
near 20% decrease in prediction error for all three baseline methods. For example, NC (RF) and NC+
7

Table 3: Prediction error (×10−2 ) on motion capture. Results averaged across 3 runs.

Error (%)

Raw
NC
NC+
4.0

10.0

3.0

8.0

Epoch Time (s)

RF

EGNN

GMN

60.9±0.9
N/A
N/A

197.0±1.0
164.8±1.7
162.6±0.8

59.1±2.1
55.8±6.1
51.3±4.0

43.9±1.1
30.0±1.4
29.6±0.8

2.0
0

1

NC(k)

2

5

0.0

1.2

18.0

1.0

16.0

0.8

14.0

0.6

12.0

Training Time (s)

SE(3)-Tr.

4.0

1.0

0.4

TFN
66.9±2.7
N/A
N/A

6.0

2.0

0.0

GNN
67.3±1.1
N/A
N/A

0

1

NC(k)

2

5

10.0

0

0

1

1

NC(k)

NC(k)

2

2

5

5

15.0
14.0
13.0
12.0
11.0
10.0
9.0

3.0
2.5
2.0
1.5
1.0
0.5
0.0

0.65

250

0.63

200

0.61

150

0.59

100

0.57
0

1

2

3

4

NC(k)

5

6

8

0.55

50
0

1

2

3

4

NC(k)

5

6

8

2.5

0.8

2.0

0.7

1.5

0.6

1.0

0.5

0.5
0

1

2

3

4

NC(k)

5

6

8

0

0.0

1

2

3

4

NC(k)

5

6

8

0.3

40000

5000

5000

5000

4000

37500

4000

4000

4000

3000

3000

3000

2000

2000

2000

35000

2000

32500

1000
0

0

1

NC(k)

2

5

(a) N-body (1, 2, 0)

30000

1000

1000
0

1

NC(k)

2

5

(b) N-body (5, 3, 3)

0

0

1

2

3

4

NC(k)

5

6

8

(c) MD17 (Aspirin)

0

1

0

1

2

3

5

6

2

3

5

6

NC(k)

0.4
0

5000
3000

0

NC(k)

GMN
RF

EGNN

1000
0

1

2

3

4

NC(k)

5

6

8

(d) MD17 (Uracil)

0

0

1

2

3

NC(k)

5

6

(e) Motion Capture

Figure 2: A detailed comparison of NCGNN without extra data on 5 datasets, w.r.t. k, average of
3 runs, conducted on a V100. The lines with dot, circle, and triangle denote the results of GMN,
EGNN, and RF, respectively. The first, second, and third rows are the results of prediction error,
training time of one epoch, and total training time, respectively.
(RF) greatly outperformed the original RF and even had better results than the original EGNN under
some settings (e.g., 3,2,1). This phenomenon also happened on the molecular dynamics datasets,
where we find that NC+ (EGNN) not only defeated its original version, but also the original GMN
and even NC+ (GMN) on many settings.
4.4

Impact of estimation step k

We conducted experiments to explore how the performance changes with respect to the estimation
step k. The experimental results are shown in Figure 2.
For all three baselines, the performance improved rapidly as the growth of step k from 0 to 2, but
this advantage gradually vanished as we set larger k. Interestingly, the curve of training time showed
an inverse trend, where we find that the total training time of NC (5) with GMN was almost two
times slower than the NC (1) version. Therefore, we set k = 2 in previous experiments to get the best
balance between performance and training cost.
By comparing the prediction errors of different baselines, we find that GMN that considers the
physical constraints (e.g., sticks and hinges) usually had better performance than the other two
methods, but the gap significantly narrowed with the increase of step k. For example, on MD17 and
motion datasets, EGNN obtained similar prediction errors for k ≥ 2 while demanding less training
time. This phenomenon demonstrates that predicting the integration with multiple estimations lowers
the requirement for model capacity.
Overall, learning with NC significantly reduced the prediction errors of different models. The
additional training time was also acceptable in most cases.
4.5

A Comparison between NC and NC+

The only difference between NC and NC+ was the use of the regularization loss in NC+ to learn the
intermediate velocities. Therefore, we designed experiments to compare the intermediate results and
the final predictions of NC and NC+ with GMN as the backbone model.
8

3.5

3.0
2.5
2.0
1.5
1.0

NC

0.5

NC+

0.0

0

3.0

NC+

2.5
2.0
1.5
1.0
0.5
0.0

Epoch 1 ,0 0 0

500

4.0

NC

Difference (%)

4.0

3.5

Difference (%)

Difference (%)

4.0

0

(a) NC(2)

500

Epoch

3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0

1, 000

NC
0

500

(b) NC(3)

Epoch

NC+

1, 000

(c) NC(5)

Figure 3: The prediction errors of intermediate velocities on valid set, w.r.t. training epoch. The blue
and green lines denote the prediction errors of NC and NC+ , respectively.

(a) NC(2) intermediate

(b) NC(3) intermediate

(c) NC(5) intermediate

(d) NC(2) final

Figure 4: Visualization of the intermediate velocities w.r.t. k. The red, blue, and green lines denote
the target, prediction of NC, and prediction of NC+ , respectively.

t=0

t=1

t=2

t=3

t=4

t=5

t=6

t=7

t=8

Figure 5: Consecutive predictions on the Motion dataset. The red, blue, and green lines denote the
target, the prediction of NC (0), and the prediction of NC+ (2), respectively. The interval between
two figures is identical to the setting in the main experiment.
We first tracked the average intermediate velocity prediction errors on valid datasets in terms of
training epochs. The results are shown in Figure 3, from which we observe that the prediction errors
were not huge for both two methods, especially NC+ obtained highly accurate prediction for the
intermediate velocities with different k. For NC, we actually find that it implicitly learned to model
the intermediate velocities more or less, even though its prediction errors gradually improved or
dramatically fluctuated during training. This observation empirically demonstrates the effectiveness
of the intermediate velocities in predicting the final state of the system. We also reported the results
on MD17 and N-body datasets in Appendix C.
We then visualized the intermediate velocities and the final predictions in Figure 4. The intermediate
velocities were visualized by adding the velocity with the corresponding coordinate at the intermediate
time point. From Figure 4, we can find that NC+ (green ones) learned very accurate predictions with
only a few non-overlapped nodes to the target nodes (red ones). For the loose version NC, it learned
good predictions for the backbone part, but failed on the limbs (especially the legs). The motion of
the main backbone was usually stable, while that of the limbs involved swing and twisting. We also
conducted experiments on the other two types of datasets, please see Figure 8 in Appendix C.
4.6

On the Time Evolution of Dynamic Systems

In previous experiments, we show that NC+ was capable of learning highly accurate predictions of
intermediate velocities. This inspired us to realize the full potential of NC+ by producing a series of
predictions given only the initial state as input. For each time point (including the intermediate time
points), we used the most recent k + 1 points of averaged data as input to predict the next velocity and
9

Table 4: Multi-step prediction results (×10−2 ) on the N-body and MD17 datasets. ADE and FDE
denote average displacement error and final displacement error, respectively. The results of four
baseline methods are from [49].
N-body (Springs)

MD17 (Aspirin)

MD17 (Benzene)

MD17 (Ethanol)

ADE

Accuracy

ADE

FDE

ADE

FDE

ADE

FDE

MD17 (Malonaldehyde)
ADE

FDE

LSTM [45]
NRI [50]
GroupNet [51]
EqMotion [49]

16.8

53.5
93.0
97.6

17.59
12.60
10.62
5.95

24.79
18.50
14.00
8.38

6.06
1.89
2.02
1.18

9.46
2.58
2.95
1.73

7.73
6.69
6.00
5.05

9.88
8.78
7.88
7.02

15.14
12.79
7.99
5.85

22.90
19.86
12.49
9.02

NC (EqMotion)

16.2

98.9

4.74

6.76

1.15

1.63

5.02

6.87

5.19

8.07

coordinate. This strategy produced much more stable predictions than the baseline NC (0) – directly
using the last output as input to predict the next state.
The results are shown in Figure 5, from which we can find that both the baseline (blue lines) and NC+
(green lines) had good predictions at the initial steps. However, due to the increasing acculturated
errors, the baseline soon collapsed and only the main backbone was roughly fit to the target. By
contrast, NC+ was still able to produce promising results. The deviations mainly concentrated on the
limbs. Therefore, we argue that NC+ can produce not only the highly accurate one-step prediction,
but also the relatively reliable consecutive predictions. We also uploaded the .gif files of examples on
all three datasets in Supplementary Materials, which demonstrated the consistent conclusion.
We also compared our method with the EGNN methods for multi-step prediction [49–52]. Specifically,
multi-step prediction takes a sequence of states with fixed interval as input and predicts a sequence of
future states with same length and interval. As recent multi-step prediction methods are still based on
sequence or encoder-decoder architectures, it would be interesting to examine the applicability of
NC to them. To this end, we adapted NC (k=2) to the source code of the best-performing method
EqMotion [49] and used identical settings for the hyper-parameters.
The experimental results are shown in Table 4, where we can observe that NC (EqMotion) outperformed the base model EqMotion on all datasets and across all metrics. It is worth noting that
the archiecture and parameter-setting of the EGNN module were identical to EqMotion and NC
(EqMotion), which clearly demonstrates the effectiveness of the proposed method. We have also
released the source code of NC (EqMotion) in our GitHub repository.
4.7

Ablation Study

We conducted ablation studies to investigate the
effectiveness of the proposed Newton-Cotes inTable 5: Ablation study results (×10−2 ).
tegration. We implemented two alternative methN-body (1,2,0) MD17 (Aspirin) Motion
ods: NC (coeffs= 1), where we set all the coef- GMN
1.84
10.14
43.9
NC (coeffs= 1)
1.78
9.64
48.4
ficients for summing the predicted velocities as
depth-varying ODE
1.82
10.08
42.2
1; and depth-varying ODE, where we employed
NC
1.55
9.50
30.0
common neural ODE solvers from [40, 41] to
address our problem. We used an identical GMN as the backend EGNN model and conducted a
simple hyper-parameter search for the learning rate and ODE solver.
The results are presented in Table 5. The two alternative methods methods generally performed
better than the baseline GMN. The performance of depth-varying GDE was comparable to that of NC
(coeffs= 1), but both were lower than that of the original NC. This observation further supports the
effectiveness of our method.

5

Conclusion

In this paper, we propose NC to tackle the problem of reasoning system dynamics. We show that the
existing state-of-the-art methods can be derived to the basic form of NC, and theoretically/empirically
prove the effectiveness of high order NC. Furthermore, with a few additional data provided, NC+ is
capable of producing promising consecutive predictions. In future, we plan to study how to improve
the ability of NC+ on the consecutive prediction task.
10

Acknowledgement
We would like to thank all anonymous reviewers for their insightful and invaluable comments. This
work is funded by NSFCU19B2027/NSFC91846204/NSFC62302433, National Key R&D Program of
China (Funding No.SQ2018YFC000004), Zhejiang Provincial Natural Science Foundation of China
(No.LGG22F030011) and sponsored by CCF-Tencent Open Fund (CCF-Tencent RAGR20230122).

References
[1] Georges Louis Leclerc comte de Buffon. Histoire naturelle, générale et particuliére: De la
manière d’étudier & de traiter l’histoire naturelle. Histoire & théorie de la terre. 1749, volume 1.
L’Imprimerie Royale, 1774.
[2] Loup Verlet. Computer" experiments" on classical fluids. i. thermodynamical properties of
lennard-jones molecules. Physical review, 159(1):98, 1967.
[3] Glenn M Torrie and John P Valleau. Nonphysical sampling distributions in monte carlo freeenergy estimation: Umbrella sampling. Journal of Computational Physics, 23(2):187–199,
1977.
[4] Daan Frenkel and Berend Smit. Understanding molecular simulation: from algorithms to
applications, volume 1. Elsevier, 2001.
[5] Frank Noé, Simon Olsson, Jonas Köhler, and Hao Wu. Boltzmann generators: Sampling
equilibrium states of many-body systems with deep learning. Science, 365(6457):eaaw1147,
2019.
[6] Ilya A Vakser. Protein-protein docking: From interaction to interactome. Biophysical journal,
107(8):1785–1793, 2014.
[7] Han Wang, Linfeng Zhang, Jiequn Han, and E Weinan. Deepmd-kit: A deep learning package
for many-body potential energy representation and molecular dynamics. Computer Physics
Communications, 228:178–184, 2018.
[8] Mieczyslaw Torchala, Iain H Moal, Raphael AG Chaleil, Juan Fernandez-Recio, and Paul A
Bates. Swarmdock: a server for flexible protein–protein docking. Bioinformatics, 29(6):
807–809, 2013.
[9] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. In ICLR, 2017.
[10] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua
Bengio. Graph attention networks. In ICLR, 2018.
[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems, 30, 2017.
[12] Jiale Liu and Xinqi Gong. Attention mechanism enhanced lstm with residual architecture and
its application for protein-protein interaction residue pairs prediction. BMC bioinformatics, 20
(1):1–11, 2019.
[13] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.
[14] Benjamin Ummenhofer, Lukas Prantl, Nils Thuerey, and Vladlen Koltun. Lagrangian fluid
simulation with continuous convolutions. In ICLR, 2019.
[15] Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and
Peter Battaglia. Learning to simulate complex physics with graph networks. In ICML, pages
8459–8468. PMLR, 2020.
[16] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W Battaglia. Learning
mesh-based simulation with graph networks. arXiv preprint arXiv:2010.03409, 2020.
11

[17] Jonas Köhler, Leon Klein, and Frank Noé. Equivariant flows: sampling configurations for
multi-body systems with symmetric energies. arXiv preprint arXiv:1910.00753, 2019.
[18] Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural
networks. In ICML, volume 139 of Proceedings of Machine Learning Research, pages 9323–
9332, 2021.
[19] Wenbing Huang, Jiaqi Han, Yu Rong, Tingyang Xu, Fuchun Sun, and Junzhou Huang. Equivariant graph mechanics networks with constraints. In ICLR, 2022.
[20] Jiaqi Han, Yu Rong, Tingyang Xu, Fuchun Sun, and Wenbing Huang. Equivariant graph
hierarchy-based neural networks. In ICLR, 2022. URL https://openreview.net/forum?
id=ywxtmG1nU_6.
[21] Bowen Dai and Chris Bailey-Kellogg. Protein interaction interface region prediction by geometric deep learning. Bioinformatics, 2021.
[22] Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using
graph convolutional networks. In Proceedings of the 31st International Conference on Neural
Information Processing Systems, pages 6533–6542, 2017.
[23] Min Zeng, Fuhao Zhang, Fang-Xiang Wu, Yaohang Li, Jianxin Wang, and Min Li. Protein–
protein interaction site prediction through combining local and global features with deep neural
networks. Bioinformatics, 36(4):1114–1120, 2020.
[24] Helen M Berman, John Westbrook, Zukang Feng, Gary Gilliland, Talapady N Bhat, Helge
Weissig, Ilya N Shindyalov, and Philip E Bourne. The protein data bank. Nucleic acids research,
28(1):235–242, 2000.
[25] Kendall Atkinson. An introduction to numerical analysis. John wiley & sons, 1991.
[26] Serge Bernstein. Sur l’ordre de la meilleure approximation des fonctions continues par des
polynômes de degré donné, volume 4. Hayez, imprimeur des académies royales, 1912.
[27] CMU. Carnegie-mellon motion capture database. 2003. URL http://mocap.cs.cmu.edu.
[28] Stefan Chmiela, Alexandre Tkatchenko, Huziel E Sauceda, Igor Poltavsky, Kristof T Schütt,
and Klaus-Robert Müller. Machine learning of accurate energy-conserving molecular force
fields. Science advances, 3(5):e1603015, 2017.
[29] Fabian Fuchs, Daniel E. Worrall, Volker Fischer, and Max Welling. Se(3)-transformers: 3d
roto-translation equivariant attention networks. In NeurIPS, 2020.
[30] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction
networks for learning about objects, relations and physics. In NeurIPS, volume 29, 2016.
[31] Damian Mrowca, Chengxu Zhuang, Elias Wang, Nick Haber, Li F Fei-Fei, Josh Tenenbaum, and
Daniel L Yamins. Flexible neural representation for physics prediction. In NeurIPS, volume 31,
2018.
[32] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter Battaglia. Learning meshbased simulation with graph networks. In ICLR, 2021. URL https://openreview.net/
forum?id=roNqYL0_XP.
[33] Solomon Kullback and Richard A Leibler. On information and sufficiency. The annals of
mathematical statistics, 22(1):79–86, 1951.
[34] Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
Improved variational inference with inverse autoregressive flow. NeurIPS, 29, 2016.
[35] Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther.
Ladder variational autoencoders. NeurIPS, 29, 2016.
[36] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural
relational inference for interacting systems. In ICML, pages 2688–2697, 2018.
12

[37] Alvaro Sanchez-Gonzalez, Victor Bapst, Kyle Cranmer, and Peter Battaglia. Hamiltonian graph
networks with ode integrators. arXiv preprint arXiv:1909.12790, 2019.
[38] Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J. Bekkers, and Max Welling.
Geometric and physical quantities improve E(3) equivariant message passing. In ICLR, 2022.
URL https://openreview.net/forum?id=_xwr8gOBeV1.
[39] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick
Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point
clouds. arXiv preprint arXiv:1802.08219, 2018.
[40] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary
differential equations. In NeurIPS, pages 6572–6583, 2018.
[41] Michael Poli, Stefano Massaroli, Junyoung Park, Atsushi Yamashita, Hajime Asama, and
Jinkyoo Park. Graph neural ordinary differential equations. arXiv preprint arXiv:1911.07532,
2019.
[42] Carl Runge. Über empirische funktionen und die interpolation zwischen äquidistanten ordinaten.
Zeitschrift für Mathematik und Physik, 46(224-243):20, 1901.
[43] Ronald J Williams and David Zipser. A learning algorithm for continually running fully
recurrent neural networks. Neural computation, 1, 1989.
[44] Lingbing Guo, Zequn Sun, and Wei Hu. Learning to exploit long-term relational dependencies
in knowledge graphs. In ICML, 2019.
[45] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9,
1997.
[46] Rafal Józefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring
the limits of language modeling. In ICLR, 2016.
[47] Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hinton.
Grammar as a foreign language. In NeurIPS, 2015.
[48] Isaac Newton. The Principia: mathematical principles of natural philosophy. Univ of California
Press, 1999.
[49] Chenxin Xu, Robby T. Tan, Yuhong Tan, Siheng Chen, Yu Guang Wang, Xinchao Wang, and
Yanfeng Wang. Eqmotion: Equivariant multi-agent motion prediction with invariant interaction
reasoning. In CVPR, pages 1410–1420. IEEE, 2023.
[50] Thomas N. Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard S. Zemel. Neural
relational inference for interacting systems. In ICML, volume 80 of Proceedings of Machine
Learning Research, pages 2693–2702. PMLR, 2018.
[51] Chenxin Xu, Maosen Li, Zhenyang Ni, Ya Zhang, and Siheng Chen. Groupnet: Multiscale
hypergraph neural networks for trajectory prediction with relational reasoning. In CVPR, pages
6488–6497. IEEE, 2022.
[52] Miltiadis Kofinas, Naveen Shankar Nagaraja, and Efstratios Gavves. Roto-translated local
coordinate frames for interacting dynamical systems. In NeurIPS, pages 6417–6429, 2021.
[53] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR,
2015.
[54] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR,
abs/1607.06450, 2016.
[55] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines.
In ICML, 2010.

13

A

The Derivations of NC (0) for Other EGNN Models

In this section, we first illustrate how the other two baseline models (i.e. RF [17] and GMN [19]) can
be derived to NC (0), and then discuss multi-layer EGNN.

A.1

RF

The overall equivariant convolution process of RF is similar to that of EGNN. The main difference
is that RF does not use the node feature. Instead, it leverages the L2 norm of the velocity as an
additional feature to update the predicted velocity:
v̂i0 = Norm(vi0 ) ϕv (hi )vi0 +


1 X 0
(xi − x0j )m0ij ,
N −1

(19)

j̸=i

x̂Ti = x0i + v̂i0 T,

(20)

where Norm(vi0 ) denotes the L2 norm of the input velocity vi0 . Therefore, RF can be also viewed as
a NC (0) model.

A.2

GMN

The main difference between GMN and EGNN is that GMN detects the sub-structures in the system
and process the particles in each special sub-structure independently. Specifically, GMN re-formulates
the original EGNN (i.e., Equations (2-4)) as follows:
m0ij = ϕe (hi , hj , ||x0i − x0j ||2 , eij ),
X
X
v̂k0 = ϕv (
hi )vk0 +
ϕk (x0i , m0ij ),
i∈Sk

(21)
(22)

i∈Sk

v̂i0 = FK(v̂k0 ),

(23)

x̂Ti = x0i + v̂i0 T,

(24)

where v̂k0 is the predicted velocity of the sub-structure. GMN then uses it to calculate the velocity
of each particle by a function FK which can be either learnable or based on the angles and relative
positions in the sub-structure [19].

A.3

Multi-layer EGNN

In current EGNN methods, the input coordinate x0i and velocity vi0 are regarded as constant feature
and used in different layers. For example, in the multi-layer version EGNN, m0ij will be formulated
as follows:
l−1
l−1
0
0 2
m0,l
ij = ϕe (hi , hj , ||xi − xj || , eij ),

(25)

l−1
0
where m0,l
denotes the hidden feature in layer l − 1. Evidently, only the
ij is mij in layer l and hi
hidden features are updated within different layers. The overall formulation of multi-layer layer is
akin to the single-layer EGNN.

14

B

Proofs of Things

B.1

Proof of Proposition 3.1

Proof. The objective of the existing methods for a single system can be defined as:
X
(xTi − x̂Ti )
argmin
v̂0

(26)

pi

=

X

(xTi − x0i − v̂i0 T )

(27)

xTi − x0i
− v̂i0 )
T

(28)

pi

=

X

T(

pi

=T

X

∗
vit − (ϕv (hi )vi0 +

0
0
0
j̸=i (xi − xj )mij

P

N −1

pi

=T

X


∗
vit − (w0 vi0 + b0 ) ,

)



(29)
(30)

pi

where w0 ∈ R1 and b0 ∈ R3 denote the learnable variables irrelevant to vi0 and t, concluding the
proof.
B.2

Proof of Proposition 3.3

Proof. As the higher order cases (k ≥ 1) have already been proved, we only need to show that
ϵNC(0) ≥ ϵNC(1) . The first order of Newton-Cotes formula NC (1) is also known as Trapezoidal rule,
i.e.:
Z T
T
v(t)dt ≈ (v0 + vT ).
(31)
2
0
As aforementioned, the actual integration xT − x0 for different training examples is different, and we
assume that it fluctuates around the base estimation T2 (v0 + vT ) and follows a normal distribution
2
NN C(1) , where the variance σN
C(1) is positively correlated with the difficulty of optimizing the
overall objective. The variance of NN C(1) is:
P RT
P1
k (k)
)dt)2
p ( 0 (v(t) −
k=0 C t
2
,
(32)
σN C(1) =
2
NT
where the integration term is a general form of polynomial interpolation error. According to Equation 14, it can be derived to:
Z T
′′
(t − t0 )(t − t1 )v (ξ)
(
)dt,
(33)
2!
0
0

′′

0
0
where v denote the second derivative of v. Let s = t−t
T , then t = t + sh and dt = d(v + sh) =
T ds. the above equation can be re-written as:
Z 1
′′
′′
s(s − 1)T 2 v (ξ)
1
T
ds = − T 3 v (ξ) = O(T 3 ).
(34)
2
12
0
2
Therefore, the final σN
C(1) is:
1 3 ′′
2
p (− 12 T v (ξ))
NT2

P
2
σN
C(1) =

4

= O(T ) ≤ O(T
concluding the proof.

15

2

2
) = σN
C(0) ,

(35)
(36)

NC+

2.5
2.0
1.5
1.0
0.5
0

3.5
3.0

NC+

2.5
2.0
1.5
1.0
0.5
0.0

500

Epoch

NC

Difference (%)

NC

3.0

0.0

4.0

4.0

3.5

Difference (%)

Difference (%)

4.0

0

3.0
2.5
2.0
1.5
1.0
0.5
0.0

50 0

Epoch

(a) NC(2)

3.5

NC
0

NC+

5 00

Epoch

(c) NC(5)

(b) NC(3)

10.0
9.0
8.0
7.0
6.0
5.0
4.0
3.0
2.0
1.0
0.0

NC
NC+

0

Difference (%)

Difference (%)

Figure 6: The prediction errors of intermediate velocities on MD17 dataset, w.r.t. training epoch. The
blue and green lines denote the prediction errors of NC and NC+ , respectively.

5 00

10.0
9.0
8.0
7.0
6.0
5.0
4.0
3.0
2.0
1.0
0.0

Epoch

NC
0

NC+

500

Epoch

(c) NC(5)

(a) NC(2)

Figure 7: The prediction errors of intermediate velocities on N-body dataset, w.r.t. training epoch.
The blue and green lines denote the prediction errors of NC and NC+ , respectively.

(a) NC(2) intermediate

(b) NC(3) intermediate

(d) NC(2) final

(c) NC(5) intermediate

Figure 8: Visualization of the intermediate velocities w.r.t. k of MD17 (Aspirin). The red, blue, and
green lines denote the target, prediction of NC, and prediction of NC+ , respectively.
B.3

Proof of Proposition 3.4

Proof. The GNN models possessing equivariance property are equivariant to the translation, rotation,
and permutation of input. NC directly feeds the input into these backbone models and naturally
possesses this property.
Formally, let Tg : x → x be a group of transformation operations. If the backbone model M is
equivariant then we will have:
M(Tg (x)) = Sg (M(x)),

(37)

where Sg is an equivalent transformation to Tg on the output space. NC can be regarded as a weighted
combination of the outputs of M:
X
X
wi M(Tg (xi ))) =
wi Sg (M(xi )),
(38)
i

i

where the Newton-Cotes weights wi is constant and irrelevant to the input, the output, and the model
M itself. Therefore, the above equation will always holds.

C

Additional Experimental Results

The average intermediate velocity prediction errors on MD17 and Motion datasets are shown in
Figure 6 and Figure 7, respectively. NC still learned the intermediate velocities we did not feed
16

Table 6: Hyper-parameter settings in the main experiments.
Datasets

k

velocity
regularization

velocity
regularization
decay

parameter
regularization

parameter
regularization
decay

loss
criterion

input
feature
normalization

intermediate
velocity
normalization

N-body
MD17
Motion

2
2
2

0.001
0.1
0.01

0.999
0.999
0.999

1.0
1.0
1.0

0.99
0.95
0.95

MSE
MSE
MSE

False
True
True

True
True
True

# epoch

batch-size

# training
examples

activation

# layers

learning
rate

optimizer

clip
gradient
norm

1,500
1,000
1,500

200
100
100

500
500
200

ReLU
ReLU
ReLU

4
4
4

0.0005
0.001
0.0005

Adam
Adam
Adam

1.0
0.1
1.0

N-body
MD17
Motion

the intermediate into it. Particularly, the error on N-body dataset was small and stable, which may
demonstrate the effectiveness of estimating intermediate velocities even without supervised data.
We also provide some visualized examples on MD17 dataset in Figure 8, from which we can observe
the similar results compared with Figure 4 in the main paper.

D

Hyper-parameter Setting

We list the main hyper-parameter setting of NC with different EGNN models on different datasets
in Table 6. We used the Adam optimizer [53] and adopted layer normalization [54] and ReLU
activation [55]) for all settings. For a fair comparison, the parameter settings for the backbone EGNN
models were identical to these in the existing implementation [19].

17

